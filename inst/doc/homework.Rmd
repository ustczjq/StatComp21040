---
title: "Homework to StatComp21040"
author: "Jiaqi Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework to StatComp21040}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

Codes of previous homework in this class.

## title: "2021—09—16"

## Question

\
**Example 1:**
\
\
(a). Generate two random number vectors of uniform distribution.
\
(b). Try to obtain normal distributed random numbers based on the first step.Plot the histograms of the generated normal ones.

\
**Example 2:**
\
\
(a). Compare the generated normal random numbers with the ones generated by function rnorm() via Q-Q plots. 
\
(b). Use Cramer-von Mises test to further verify if we successfully generate random numbers of normal distribution.

\
**Example 3:**
\
\
Implement a linear regression on the dataset *trees* and see if the linear form is suitable.
\
\

## Answer
\

**Example 1:**

\
First,generate two random number vectors $U$,$V$.
\

```{r}
set.seed(7725)
U <- runif(1000,0,1)          # U ~ U[0,1]
V <- runif(1000,0,1)          # v ~ U[0,1]
```


As $U,V \sim U[0,1]$,we can further generate random numbers from normal distribution:
\
\
<center>$Z_{1}=\sqrt{-2logU}cos(2\pi V)\sim N[0,1]$</center>
\
<center>$Z_{2}=\sqrt{-2logU}sin(2\pi V)\sim N[0,1]$</center>
\
*Proof:*
\ 
Solve $U,V$ through $Z_{1},Z_{2}$ ,the Jacobian $J$ yields as:
\

<center>$J=\frac{\partial (U,V)}{\partial (Z_1,Z_2)}=\begin{vmatrix}
\frac{\partial U}{\partial Z_1} & \frac{\partial U}{\partial Z_2}\\ 
\\
\frac{\partial V}{\partial Z_1}& \frac{\partial V}{\partial Z_2}
\end{vmatrix}=-\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \cdot   \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}$</center>

Therefore,
<center>$f_{Z_1,Z_2}(x,y)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \cdot   \frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}},x,y \in R$</center>
\
<center>$Z_{1},Z_{2}\sim N[0,1]$</center>

\
Generate normal random numbers and plot the histograms：
\

```{r}
m<-sqrt(-2*log(U))*cos(2*pi*V)
n<-sqrt(-2*log(U))*sin(2*pi*V)
par(mfrow=c(1,2))
hist(m,main = "Histogram of Z1")
hist(n,main = "Histogram of Z2")

```

The histograms show highly possible normality. 

\

**Example 2:**

\
Use  Q-Q plots to compare the generated normal random numbers with the ones generated by function rnorm(). 
\
```{r}
r<- rnorm(1000,0,1)
par(mfrow=c(1,2))
qqplot(r,m,ann="FALSE")
mtext("Normal Q-Q plots for U",adj=0.5,line=1)
qqplot(r,n,ann="FALSE")
mtext("Normal Q-Q plots for V",adj=0.5,line=1)

```

As the curves are close to straight lines,we can regard the generated randoms are from normal contribution $N(0,1)$.
\

In spite of the strong visuality,the methods already presented are both subject methods.Therefore,we consider objective methods like Cramer-von Mises test to verify.
\

```{r}
library(nortest)
cvm.test(m)
cvm.test(n)
```
\
Considering the randomness,the relatively low p-values verify the feasibility to generate normal random numbers from uniform ones.  

\

**Example 3:**

\
First of all,introduce the dataset *trees* ,which contains 31 observations on 3 variables.
\
\
Variables used in the linear regression are:
\
Girth：numeric diameter in inches
\
Height：numeric	Height in ft
\
Volume：numeric	Volume of timber in cubic ft

Have a qucik glimpse of the dataset trees and show the main components as following table:

```{r}
knitr::kable(head(trees))     
```

Implement the linear regression in the form: $Y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}$.

```{r}
lmtrees <- lm(Volume~Girth+Height,data=trees)
summary(lmtrees)$coef
 
```

Therefore we have regression:$Volume=-57.9876589+ 4.7081605 Girth+0.3392512 Height$.


Use plot(lm) to check whether the linear regression is suitable or not.
\


```{r}
plot(lmtrees)  
 
```

\
**The above four figures contain useful information:**
\

The first figure indicates that a model-correction can be considered since residuals(real value - fitted value) and fitted value are correlated(quadratic relationship).

Linear regression requires the normality of residual.The second figure manifests that the residuals are approximately normal distributed.  

Equal variance hypothesis is also needed for regression,which is verified in the third picture.

The last figure indicates possible outliers and influential points(Cook's distance > 0.5),the 31th observation should be studied carefully.

\
\

## title: "2021—09—23"

## Question

\
**3.4**
\
The Rayleigh density is $f(x) = \frac{x}{\sigma ^2}e^{-\frac{x^2}{2\sigma ^2}},\  x\geq 0,\sigma\geq 0$
Develop an algorithm to generate random samples from a Rayleigh$(\sigma)$ distribution. Generate Rayleigh$(\sigma)$ samples for several choices of $\sigma > 0$ and check
that the mode of the generated samples is close to the theoretical mode $(\sigma)$
(check the histogram).

\
**3.11**
\

Generate a random sample of size 1000 from a normal location mixture. The
components of the mixture have $N(0, 1)$ and $N(3, 1)$ distributions with mixing
probabilities $p_1$and $p_2=1-p_1$. Graph the histogram of the sample with
density superimposed, for $p_1 = 0.75$. Repeat with different values for p1
and observe whether the empirical distribution of the mixture appears to be
bimodal. Make a conjecture about the values of$p_1$ that produce bimodal
mixtures.


\
**3.20**
\
*A compound Poisson process* is a stochastic process ${X(t), t ≥ 0}$ that can be
represented as the random sum $X(t) =\sum_{i=1}^{N(t)}Y_i,\ t\geq 0$,where $N(t),\ t\geq 0$ is a Poisson process and $Y1, Y2$,... are iid and independent of $N(t),\ t\geq 0$.
Write a program to simulate a compound Poisson($\lambda$)–Gamma process ($Y$ has a Gamma distribution). Estimate the mean and the variance of $X(10)$ for
several choices of the parameters and compare with the theoretical values.
Hint: Show that $E[X(t)] = λtE[Y_1]$ and $Var(X(t)) = λtE[Y_1^2]$.
\

## Answer
\

**3.4:**
\
For continuous situation,we can use the inverse transition method:  
\
As $f(x) = \frac{x}{\sigma ^2}e^{-\frac{x^2}{2\sigma ^2}},\  x\geq 0$,\ $F(x)=1-e^{-\frac{x^2}{2\sigma ^2}}$
\
Then,$U=F_X(x)\sim U(0,1)$
\
Because $x\geq 0$,we have $x=\sqrt{2\sigma^2 ln^{\tfrac{1}{1-u}}}$,which should be Rayleigh distributed.
\
Set $\sigma=1,5,15$,respectively.
\
Plot the histograms.
\

```{r}
set.seed(2021)

U <- runif(1000,0,1)        
C1<-1                                 # sigma = 1
x<- sqrt(-2*C1^2*log(1-U)) 
hist(x,probability=TRUE,main=expression(f(x)==x%.%exp(-frac(x^2,2))))
z1<-seq(0,4,0.01)
lines(z1,z1*exp(-z1^2/2),col="red")

C2<-5                                 # sigma = 5
y<- sqrt(-2*C2^2*log(1-U)) 
hist(y,probability=TRUE,main=expression(f(x)==frac(x,25)%.%exp(-frac(x^2,50))))
z2<-seq(0,20,0.05)
lines(z2,z2/25*exp(-z2^2/50),col="red")


C3<-15                                # sigma = 15
w<- sqrt(-2*C3^2*log(1-U)) 
hist(w,probability=TRUE,main=expression(f(x)==frac(x,225)%.%exp(-frac(x^2,500))))
z3<-seq(0,55,0.1)
lines(z3,z3/225*exp(-z3^2/500),col="red")
```

\
The histograms show that the mode of the generated samples is close to the theoretical mode $\sigma$.
\
\
**3.11:**
\
\
Generate two random samples $X_1 \sim N(0,1),X_2 \sim N(3,1)$.
\
Mix them with probability $p_1$ and $p_2$,respectively to obtain a mixture $Z$,$Z=p_1X_1+p_2X_2$. 
\
Firstly,consider $p_1=0.75,p_2=0.25$.
\
```{r}
set.seed(2021)
n<-1000
X1<-rnorm(n,0,1)
X2<-rnorm(n,3,1)

#Use sample method to simulate p1=0.75
r1<- sample(c(1,0),n,replace = TRUE,prob = c(0.75,0.25))
Z1<-r1*X1+(1-r1)*X2
hist(Z1,prob=TRUE,xlim = c(-5,9),ylim = c(0,0.28),main="p1=0.75,p2=0.25")
lines(density(Z1),lwd = 2, col = "blue")

```
\
The center of the figure is at around 0,which is the mean of $X_1$.The considerable proportion of $p_1$ may be accountable for this.  
\
Consider the counterpart form $p_1=0.25,p_2=0.75$.
\
```{r}
r2<- sample(c(1,0),n,replace = TRUE,prob = c(0.25,0.75))
Z2<-r2*X1+(1-r2)*X2
hist(Z2,prob=TRUE,xlim =  c(-5,9),ylim = c(0,0.3),main="p1=0.25,p2=0.75")
lines(density(Z2),lwd = 2, col = "blue")
```
\
The center of the figure is at around 3,which is the mean of $X_2$.The considerable proportion of $p_2$ may be accountable for this.
\
\
Finally,let $p_1=0.5,p_2=0.5$.
\
```{r}

r3<- sample(c(1,0),n,replace = TRUE,prob = c(0.5,0.5))
Z3<-r3*X1+(1-r3)*X2
hist(Z3,prob=TRUE,xlim = c(-5,9),ylim = c(0,0.25),main="p1=0.5,p2=0.5")
lines(density(Z3),lwd = 2, col = "blue")

```

\
The center of the figure is at around 2.
\
Besides,the empirical distribution of the mixture appears to be bimodal.This happens when the mixture proportion is equal.
\
\
**Conjecture：**
\
So we can speculate that for the normal distribution with different location parameter,but with the same shape parameter,**the equal proportion (i.e. $p_1=p_2$)situation** is most likely to yield a bimodal mixture. 
\
\

**3.20:**
\
\

$X=\sum_{i=1}^{N(t)}Y_i$,where $N(t) \sim Possion(\lambda t),Y \sim \Gamma(\alpha ,\beta )$
\
\
$E(N)=Var(N)=\lambda t,E(Y)=\frac{\alpha}{\beta},Var(Y)=\frac{\alpha}{\beta^2}$
\
\
We can obtain $E(X)$ and $Var(X)$ through the conditional expectation and conditional variance as follows:

$$\begin{split}
E(X)&=E[E(\sum_{i=1}^{N}Y_i)|N]\\
& =E(NE(Y_1))\\
& =ENE(Y_1)& \\
& =\lambda tE(Y_1)
\end{split}$$
\

$$\begin{split}
Var(X)&=Var(E(X|N))+E(Var(X|N))  \  \  (*)\\
&=E(E[(\sum_{i=1}^{N}Y_i)^2]|N) \\
&=E(N)Var(Y_1)+E(N^2)[E(Y_1)^2]$ \\
&=Var(NE(Y_1))+E(NVar(Y_1)) \\
&=E^2(Y_1)Var(N)+Var(Y_1)E(N)\\
&=\lambda tE(Y_1^2)
\end{split}$$
\

$$\begin{split}
(*):\ Var(X)&=E(X-EX)^2 \\
&=E(X-E(X|N)+E(X|N)-EX)^2 \\
&=E[E(X-E(X|N))^2|N]+Var(E(X|N))\\
&=E(Var(X|N))+Var(E(X|N))
\end{split}$$

\
Here is the program to simulate the compound Poisson–Gamma process.
\
Firstly,define a function CPG to try different value combinations.
\
(Note that $t=10$ is fixed,other parameters can be changed.)
\

```{r}

# Compound Poisson–Gamma process

CPG<-function(n,t,lamb,alpha,beta){
N<-rpois(n,lamb*t)
Z<-c()
for (i in 1:n){
    Z[i]<-sum(rgamma(N[i],alpha,beta));
}


print(paste("The mean of X is ",mean(Z),",which is very close to the theoretical value ",lamb*t*alpha/beta))
print(paste("The variance of X is ",var(Z),",which is very close to the theoretical value ",lamb*t*((alpha/beta)^2+(alpha/beta^2))))
}


#Different parameter value combinations:
CPG(1000,10,1,2,2)          
CPG(2000,10,3,4,4)
CPG(1500,10,0.5,1,1)
```
\
We can see that for several parameter combinations,both the mean and the variance of $X(10)$ are close to the theoretical values.

\
\
## title: "2021—09—30"

## Question

\
**5.4**
\
Write a function to compute a Monte Carlo estimate of the $Beta(3, 3)$ cdf,
and use the function to estimate $F(x)$ for $x = 0.1, 0.2,..., 0.9$. Compare the
estimates with the values returned by the pbeta function in R.

\
**5.9**
\
The Rayleigh density is $f(x) = \frac{x}{\sigma ^2}exp({-\frac{x^2}{2\sigma ^2}}),\  x\geq 0,\sigma\geq 0$.
 \
Implement a function to generate samples from a Rayleigh $(σ)$ distribution,using antithetic variables. What is the percent reduction in variance of  $\frac{X+{X}'}{2}$ compared with $\frac{X_1+X_2}{2}$ for independent $X1, X2$?
 

\
**5.13**
\
Find two importance functions $f_1$ and $f_2$ that are supported on $(1,+\infty)$ and
are ‘close’ to $$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},\ x>1$$
Which of your two importance functions should produce the smaller variance
in estimating
$\int_{1}^{\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx$
by importance sampling? Explain.

\

**5.14**
\
Obtain a Monte Carlo estimate of 
$\int_{1}^{\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx$
by importance sampling.
\



## Answer
\

**5.4:**
\
\

Use the Monte Carlo method to estimate $F(x)$ of $Beta(3,3)$:
\

$$\begin{split}
F(x)&=\int_{0}^{x}\frac{\Gamma (6)}{\Gamma (3)\Gamma (3)}t^2(1-t)^2dt,\ x\in [0,1] \\
&=\int_{0}^{x}30t^2(1-t)^2dt \\
&=\int_{0}^{x}x\cdot 30t^2(1-t)^2 \cdot \frac{1}{x}dt \\
&=30x\cdot E(X^2(1-X)^2) ,\ X\sim U[0,1] \\
\end{split}$$
\
```{r}
n<-10000
x<-seq(from=0.1,to=0.9,by=0.1) 
set.seed(2021)
u <- numeric(n)
E<- numeric(9)

for (i in 1:9){
u <- runif(n,0,x[i])
E[i]<- round(mean(x[i]*30*u^2*(1-u)^2),5)   #
}

F<-pbeta(x,3,3)
print(paste("The Monte Carlo estimate of Beta(3,3) cdf F(x) for x=",x," is ",E,",while the value returned by the pbeta function in R is ",F))

```

\
The estimated values are close to the ones returned by pbeta(),which means the estimation is satisfactory.
\
\
\
**5.9:**
\
\
As $f(x) = \frac{x}{\sigma ^2}e^{-\frac{x^2}{2\sigma ^2}},\  x\geq 0$,\ $F(x)=1-e^{-\frac{x^2}{2\sigma ^2}}$.
Then,$U=F_X(x)\sim U(0,1)$.
\
Because $x\geq 0$,we have $x=\sqrt{-2\sigma^2 ln^{1-u}}$,which should be Rayleigh distributed.
\
\
Then we use antithetic variables to generate samples from Rayleigh distribution.

\


```{r}
set.seed(2021)
MCR<- function(s=1,N=1000,antithetic=TRUE) {   # s means sigma
  u <- runif(N/2,0,1)              # half random numbers from U[0,1]
  if (!antithetic) 
    v <- runif(N/2,0,1)     # antithetic=False,i.e.the original situation
  else 
    v <- 1 - u
  
  u <- c(u, v)
  X<-mean(s*sqrt(-2*log(1-u)))
  X
}


m <- 1000
MCR1<- MCR2<- numeric(m)


for (i in 1:m) {
  MCR1[i]<-MCR(s=2,N=1000,anti=FALSE)    #Original method
  MCR2[i]<-MCR(s=2,N=1000,anti=TRUE)     #Antithetic method
}

VarianceRatio<-round(var(MCR2)/var(MCR1),4)
print(paste("The ratio of variance between two methods is",VarianceRatio,"and the percent reduction in variance is about",round((var(MCR1)-var(MCR2))/var(MCR1),4)*100,"%"))
  

```

\
Therefore,we can draw the conclusion that the antithetic method can greatly reduce the variance.
\
\
\
**5.13:**
\
\
We choose $f_1(x)=exp(-(x-1)),x\in(1,+\infty),f_2(x)=\frac{4}{\pi(1+x^2)},x\in(1,+\infty)$ as importance functions.
\
\
Now plot the curves of $g(x),f_1(x),f_2(x)$.They are printed in red,blue and green,respectively.
\
```{r}
 g<-function(x) 
   {x^2/sqrt(2*pi)*exp(-x^2/2)*(x>=1)}  
 f1<-function(x) 
   {exp(-x+1)}     
 f2<-function(x) 
 {4/pi/(1+x^2)}          

 
 x<-seq(1,5,0.01)
gf1<-c(expression(g(x)==frac(x^2,sqrt(2*pi))*exp(-frac(x^2,2))),expression(f1(x)==exp(-(x-1))),expression(f2(x)==frac(4,pi*(1+x^2))))

 plot(x,g(x), type = "l", ylab = "",ylim = c(0,1), lwd = 0.3,col="red",main='The plots of g and f')
 lines(x, f1(x), lty = 2, lwd = 0.3,col="blue")
 lines(x, f2(x), lty = 3, lwd = 0.3,col="green")
 legend("topright", legend = gf1, lty = 1:3, col=c("red","blue","green"),lwd = 0.2,cex=1.2)
```
\
Further,plot the curves of $\frac{g(x)}{f_1(x)},\frac{g(x)}{f_2(x)}$.
\
```{r}
gf2<-c(expression(g(x)/f1(x)),expression(g(x)/f2(x)))

 plot(x,g(x)/f1(x), type = "l", ylab = "",ylim = c(0,1), lwd = 0.3,col="blue",main='The plots of g/f')
 lines(x, g(x)/f2(x), lty = 2, lwd = 0.3,col="green")
 legend("topright", legend = gf2, lty = 1:2, col=c("blue","green"),lwd = 0.2,cex=1.2)
```
\
We can believe that $f_1(x)$ would surpass $f_2(x)$ for better simulation to $g(x)$ because compared with $\frac{g(x)}{f_2(x)}$,$\frac{g(x)}{f_1(x)}$ is more nearly to be a constant,leading to a better estimation.
Use $f_1(x)$ and $f_2(x)$ to estimate and compare.
\

```{r}
set.seed(100)
N<-10000
u<-runif(N,0,1)
x<-1-log(1-u)
y<-tan(pi/4*(u+1))
integral1<-mean(g(x)/f1(x))
integral2<-mean(g(y)/f2(y))
Real<-integrate(g,1,Inf)
print(paste("The estimate of the integral using f1 is ",round(integral1,5),"and the real value is ",round(Real$value,5)))
print(paste("The estimate of the integral using f2 is ",round(integral2,5),"and the real value is ",round(Real$value,5)))
print(paste("The standard variance of estimation using f1 is ",round(sd(g(x)/f1(x)),5),"and the one using f2 is ",round(sd(g(x)/f2(x)),5)))
```

\
The result shows that $f_1$ is a better choice,which corroborates our judgment. 

\
\
**5.14:**
\
\
Note that $$\begin{split}
\int_{-\infty }^{+\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx&=E(X^2),X\sim N(0,1) \\
&=Var(X)+[E(X)]^2=1
\end{split}$$
Besides,the integrand function is an even function,so $\int_{-\infty }^{+\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=2\int_{0}^{+\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx$.
Therefore,$\int_{1}^{\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=\frac{1}{2}-\int_{0}^{1 }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx$.
\
\
Then we can only consider the interval $[0,1]$ rather than $[1,+\infty]$,which can improve the efficiency.
\
\
Now introduce the choice of importance functions:
\
\
First,we choose $f_1(x)=\frac{e^{-x}}{1-e^{-1}},x\in(0,1)$,which is the truncated distribution of  $f(x)=e^{-x},x\in(0,+\infty)$.
\
Then we set $f_2(x)=\frac{xe^{-\frac{x^2}{2}}}{1-e^{-\frac{1}{2}}},x\in(0,1)$,which is the truncated distribution of  $f(x)=xe^{-\frac{x^2}{2}},x\in(0,+\infty)$.
\
\
Note that $\int_{-\infty }^{+\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=E(\frac{g(X)}{f_2(X)}),X \sim Rayleigh(1)$.
\
So $\int_{1}^{\infty }\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=\frac{1}{2}-\frac{1-e^{-\frac{1}{2}}}{\sqrt{2\pi}} \int_0^1xf(x)dx$,where $f(x)=e^{-\frac{x^2}{2}}$ is the pdf of $Rayleigh(1)$.This will be used in following estimation.
\
(Use the inverse method ($x=\sqrt{-2ln^{1-u}}$) to generate random numbers from $Rayleigh(1)$.)
\
\
Now plot the curves of $g(x),f_1(x),f_2(x)$.They are printed in red,blue and green,respectively.
\
```{r}
 g<-function(x) 
   {x^2/sqrt(2*pi)*exp(-x^2/2)}
 f3<-function(x) 
   {exp(-x)/(1-exp(-1))}
 f4<-function(x) 
 {x/(1-exp(1)^(-1/2))*exp(-x^2/2)}
 
  x<-seq(0,1,0.001)
  
 gf3<-c(expression(g(x)==frac(x^2,sqrt(2*pi))*exp(-frac(x^2,2))),expression(f1(x)==frac(exp(-x),(1-exp(-1)))),expression(f2(x)==frac(x*exp(-frac(x^2,2)),(1-e^(-frac(1,2))))))

 plot(x,g(x), type = "l", ylab = "",ylim = c(0,1.5), lwd = 0.3,col="red",main='The plots of g and f')
 lines(x, f3(x), lty = 2, lwd = 0.3,col="blue")
 lines(x, f4(x), lty = 3, lwd = 0.3,col="green")
 legend("topleft", legend = gf3, lty = 1:3, col=c("red","blue","green"),lwd = 0.2,cex=0.9)
```
\
Clearly,We can see $f_2(x)$ and $g(x)$ share the same increasing trend,while $f_1(x)$ is on the contrary.Further,plot the curves of $\frac{g(x)}{f_1(x)},\frac{g(x)}{f_2(x)}$.
\

```{r}
gf4<-c(expression(g(x)/f1(x)),expression(g(x)/f2(x)))

 plot(x,g(x)/f3(x), type = "l", ylab = "",ylim = c(0,0.2), lwd = 0.3,col="blue",main='The plots of g/f')
 lines(x, g(x)/f4(x), lty = 2, lwd = 0.3,col="green")
 legend("topleft", legend = gf4, lty = 1:2, col=c("blue","green"),lwd = 0.2,cex=1.2)
```
\
We can believe that $f_2(x)$ would surpass $f_1(x)$ for better simulation to $g(x)$ because compared with $\frac{g(x)}{f_1(x)}$,$\frac{g(x)}{f_2(x)}$ is more nearly to be a constant,leading to a better estimation.
Use $f_1(x)$ and $f_2(x)$ to estimate and compare.
\
\
```{r}
set.seed(2021)
N<-10000
u<-runif(N,0,1)
z<--log(1-u*(1-exp(-1)))
integral3<-1/2-mean(g(z)/f3(z))

x<-sqrt(-2*log(1-u))         #Generate Rayleigh random numbers 
y<-x[x<=1]                   #Select the random numbers locating on [0,1]
integral4<-1/2-(1-exp(-1/2))/sqrt(2*pi)*mean(y)

print(paste("The estimate of the integral using f1 is ",round(integral3,5),"which is close to the real value",round(Real$value,5)))
print(paste("The estimate of the integral using f2 is ",round(integral4,5),"which is close to the real value",round(Real$value,5)))
```
So we can use $f_2$ for estimation. And the estimate value of integral is 0.40071 while the real value is about 0.40063.


\
\

## title: "2021—10—14"

## Question

\
**6.5**
\
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)

\
**6.A**
\
Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2(1)$, (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $$H_0 : \mu=\mu_0 \quad vs \quad H_1 : \mu \neq\mu_0,$$where $µ_0$ is the mean of $\chi^2(1)$, Uniform(0,2), and Exponential(1), respectively.
 

\
**Discussion**
\
If we obtain the powers for two methods under a particular
simulation setting with 10,000 experiments: say, 0.651 for one
method and 0.676 for another method. We want to know if the
powers are different at 0.05 level.
What is the corresponding hypothesis test problem?
What test should we use? Z-test, two-sample t-test, paired-t
test or McNemar test? Why?
Please provide the least necessary information for hypothesis
testing.

\


## Answer
\

**6.5:**
\
\
If $X \sim N(\mu,\sigma^2),n\geq 2$,then $\frac{X-\mu}{\sigma} \sim N(0,1)$,$\frac{(n-1)S^2}{\sigma^2}\sim \chi ^2(n-1)$,$\frac{X-\mu}{S} \sim t(n-1)$.

Use non-normal random samples (in this example,$X\sim \chi ^2(2)$),to see how the estimation result will change.
\
Note that a 95% symmetric t-interval is constructed. 
\
```{r}
#t-interval
set.seed(2021)
n<-20
alpha <- 0.05
N<-1e3
Lower<-Upper<-numeric(N)

for(i in 1:N)
{
    x<-rchisq(n, 2) 
    Lower[i]=mean(x)-qt(alpha/2,df=n-1,lower.tail = FALSE)*sd(x)/sqrt(n) 
    #lower.tail = FALSE calculates the upper quantile  
    Upper[i]=mean(x)+qt(alpha/2,df=n-1,lower.tail = FALSE)*sd(x)/sqrt(n)
}

print(paste("The estimation of the coverage probability of the t-interval is",mean(Lower<2 & Upper>2),"while the nominal level is ",1-alpha))  
```
The estimation of the coverage probability is visibly smaller than the nominal level. 
\
Compared with the result of example 6.4, which shows that **if the sampled population is normal with variance $σ^2$, then the probability that the confidence interval contains $σ^2$ is $1−\alpha$**,the reason why the estimation of CP is liberal should be that the samples are non-normal.
\
```{r}
#variance interval
set.seed(2021)
n<-20
alpha <- 0.05
Upper2<-numeric(N)
for(i in 1:N)
{
    x <- rchisq(n, 2) 
    Upper2[i] <- (n-1) * var(x) / qchisq(alpha, df=n-1)
}
print(paste("The estimation of the coverage probability of variance interval is",mean(Upper2>4),"while the nominal level is ",1-alpha))  
```

What's more,the coverage probability for t-interval is approximately 92% ,which is greater than the interval for variance (78.5%).
\
Therefore,we can draw the conclusion that t-interval is more robust to departures from normality than the interval for variance.
\
\
**6.A:**
\
\
As the means of three distributions($\chi^2(1),U[0,1],Exp(1)$) are all 1,so the hypothesis test is:\ 
$$H_0:\mu=1 \quad vs \quad H_1:\mu\neq 1$$
\
Different sample sizes(20,50,250,500) are considered in simulations.
\
```{r}
set.seed(2021)
N<-10000
num<-c(20,50,250,500)
alpha<-0.05  
mu<-1
M<-numeric(N)

results <- matrix(0,nrow = length(num),ncol = 4)
colnames(results) <- c("sample size","chi square(1)","U[0,2]","Exp(1)")

for (n in num){
  quant<-qt(1-alpha/2,n-1)
for (i in 1:N) {
 X<-rchisq(n,1)
 T<-abs(sqrt(n)*(mean(X)-mu)/sd(X))
 M[i]<-as.integer(T>quant)
 
}
results[which(num == n),2]<-mean(M)


for (i in 1:N) {
 X<-runif(n,0,2)
T<-abs(sqrt(n)*(mean(X)-mu)/sd(X))
 M[i]<-as.integer(T>quant)
}
results[which(num == n),3]<-mean(M)

for (i in 1:N) {
 X<-rexp(n,1)
 T<-abs(sqrt(n)*(mean(X)-mu)/sd(X))
 M[i]<-as.integer(T>quant)
}
results[which(num == n),4]<-mean(M)
}

for (p in 1:length(num)){
  results[p,1]<-num[p]
  }


library(knitr)
knitr::kable(results,align=rep('c', 5))
```

\
We can see that the empirical Type I error rates of the t-test are approximately equal to the nominal significance level 0.05,when the sampled population is non-normal.And as sample size increases,the estimation tends to be more accurate.So t-test is robust to mild departures from normality.

\
\
**Discussion:**
\
\
(1)The hypothesis test problem (to test if the powers are different) is:\ 
$$H_0:p_1=p_2 \quad vs \quad p_1\neq p_2$$
\
(2) We  can generate two binomial distributions $b(n,p_1)$ and $b(n,p_2)$,then use the mean value of  $b(n,p_i)$ to estimate $p_i,i=1,2$.
For each simulation, the outcome is a binomial experiment.
\
Considering the powers obtained by two methods must be strongly correlated,(i.e. the two binomial samples are not independent),the two-sample t-test which requires independence should be discarded,and we can use the paired-t test.
\
\
For paired-t test,under $H_0$,$T=\frac{\sqrt{n}(\bar{d}-\mu_d)}{S_d}$,where $\bar{d}=\bar{x}-\bar{y},\mu_d=\mu_1-\mu_2$.
\
(3)The feasibility of paired-t test may be demonstrated as follows:
\
Note that this is a two-side test.
\
```{r}
set.seed(2021)
N<-10000
n<-100
alpha<-0.05
p1<-0.651
p2<-0.676
quant<-qt(1-alpha/2,n-1)
M<-numeric(N)
for (i in 1:N) {
  x<-rbinom(n,1,p1)
  y<-rbinom(n,1,p2)
  T<-(mean(x-y))/sqrt((var(x-y))/n)
  M[i]<-as.integer(T>quant)
}
print(paste("the p-value is",2*mean(M),"while the significance level is",alpha))
```
So at significance level $0.05$,we can refuse the null hypothesis,which means that the powers are not the same at 0.05 level.

\
\
## title: "2021—10—21"

## Question

\
**6.C**
\
\
Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test.Mardia proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis.If $X$ and $Y$ are $i.i.d.$,the multivariate population skewness $\beta_{1,d}$ is defined by Mardia as$$\beta_{1,d}=E[(X-\mu)^T\Sigma^{-1}(Y-\mu)]^3$$.
\
Under normality,$\beta_{1,d}=0$.The multivariate skewness statistic is
$$b_{1,d}=\frac{1}{n^2}\sum_{i,j=1}^{n}((X_i-\bar{X})^T\hat{\Sigma}^{-1}(X_j-\bar{X}))^3,$$
\
where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1,d}$ are significant.The asymptotic distribution of $\frac{nb_{1,d}}{6}$ is chisquared with $\frac{d(d+1)(d+2)}{6}$ degrees of freedom.


\


## Answer
\

**6.C**
\
\
**Skewness test of normality**
\
\
Different sample sizes are considered in simulation(n=10,50,500).
```{r}
library(MASS)
set.seed(2021)
d<-3                       #dimension 
num<-c(10,50,500)          #sample size
m<-500                    #simulation times
alpha<-0.05                #significance level
R <- diag(rep(1,d))        #required in function mvnorm() as Sigma 
skk<- numeric(m) 
criticalvalue<-qchisq(1-alpha,d*(d+1)*(d+2)/6)

#function to get b1d.
skew<-function(X) {
Xbar<-colMeans(X)
Sigma<-cov(X)
b<-numeric(n)
  for (i in 1:n) {
    b[i]<-mean(((X[i,]-Xbar)%*%ginv(Sigma)%*%t((X-Xbar)))^3)
  }

return(mean(b)*n/6)

}
#The first loop is to produce results for different sample size,and the second is used for m simulations.
for (n in num){
for (k in 1:m) {
  X<-mvrnorm(n,mu=rep(0, d),Sigma = R)
  
  #Or generate X,Y in this way:
  #X<-matrix(rnorm(n*d),nrow=n,ncol=d)
 
  
  skk[k] <- as.integer(skew(X)>=criticalvalue)
}
p<-mean(skk)
print(paste("The critical probability for n=",n,"is",p))
}

```
We can see that as n increases, the sample proportion of significant tests tends to be 0.05,which is the significance level.


\
\
**Power of the skewness test of normality**
\
\
Now consider the power.
\
```{r}
library(MASS)
set.seed(2021)
alpha<-0.05    #significance level
n<-30          #sample size  
m<-300         #simulation times
d<-3           #dimension 
criticalvalue2<-qchisq(1-alpha,d*(d+1)*(d+2)/6)
eps<-c(seq(0,0.15,0.01), seq(0.15,1,0.05))
N<-length(eps)
power<-numeric(N)    
skk<-numeric(m)

#function to get b1d.
skew<-function(X) {
Xbar<-colMeans(X)
Sigma<-cov(X)
b<-numeric(n)
  for (i in 1:n) {
    b[i]<-mean(((X[i,]-Xbar)%*%ginv(Sigma)%*%t((X-Xbar)))^3)
  }
return(mean(b)*n/6)
}


#for each epsilon,produce the contaminated probability.
for (j in 1:N) { 
e <- eps[j]

#The loop is produced for m simulations.
for (i in 1:m) { 

 sigma<-sample(c(1, 10), replace = TRUE,size = n*d, prob = c(1-e, e))
 X<-matrix(rnorm(n*d,0,sigma),nrow=n,ncol=d)
 skk[i]<-as.integer(skew(X)>=criticalvalue2)
}
 power[j] <- mean(skk)
}


#plot power vs epsilon
plot(eps, power, type = "b",
xlab = bquote(epsilon), ylim = c(0,1))
abline(h=alpha,lty =3,col="red")
se<-sqrt(power*(1-power)/m)     
lines(eps,power+se,lty=3)
lines(eps,power-se,lty=3)
```

The power curve crosses the horizontal line corresponding to $\alpha=0.05$ at both endpoints:$\epsilon =0$ and $\epsilon =1$.
\
Besides,for $0<\epsilon<1$,the empirical power of the test is greater than $\alpha=0.05$ and reaches its peak when $\epsilon$ falls in [0.1,0.15].
\
\
---
title: "Homework to StatComp21040"
author: "Jiaqi Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework to StatComp21040}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## title: "2021—10—28"

## Question

\
**7.7**
\
Refer to Exercise 7.6. Efron and Tibshirani discuss the following example The five-dimensional scores data have a $5 × 5$ covariance matrix $\Sigma$,
with positive eigenvalues $\lambda_1 > ··· > \lambda_5$. In principal components analysis,
$$\theta =\frac{\lambda _1}{\sum_{j=1}^{5}\lambda _j}$$
measures the proportion of variance explained by the first principal component. Let $\hat{\lambda_1} > ··· > \hat{\lambda_5}$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$is the MLE of  $\Sigma$. 
Compute the sample estimate
$$\hat{\theta} =\frac{\hat{\lambda _1}}{\sum_{j=1}^{5}\hat{\lambda _j}}$$
Bootstrap and Jackknife 213
of$\theta$. Use bootstrap to estimate the bias and standard error of $\hat{\theta}$.

\
**7.8**
\
Refer to Exercise 7.7.Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$.

\
**7.9**
\
Refer to Exercise 7.7.Compute 95% percentile and BCa confidence intervals
for $\hat{\theta}$.

\

**7.B**
\
Repeat Project 7.A for the sample skewness statistic. Compare the coverage
rates for normal populations (skewness 0) and $\chi ^{2}(5)$ distributions (positive skewness).
\
\


## Answer
\

**7.7:**
\
\
Use Bootstrap method to estimate bias and standard error:
\
```{r}
set.seed(2021)
library(bootstrap)     #obtain scor data
library(boot)          #use function boot()
lambdaesti<-eigen(cov(scor))$values
thetaesti<-lambdaesti[1]/sum(lambdaesti)
N<-400                #number of bootstrap samples
SCOR<-cbind(scor$mec,scor$vec,scor$alg,scor$ana,scor$sta)


#Bootstrap
bs<-function(data,index){
x<-data[index,]
lambda<-eigen(cov(x))$values
theta<-lambda[1]/sum(lambda)
return(theta)
}

bsr<-boot(data=SCOR,statistic=bs,R=N)

thetabs<-bsr$t
biasbs<-mean(thetabs)-thetaesti
sebs<-sd(thetabs)


results<-matrix(0,nrow=1,ncol=2)
rownames(results)<-c("Bootstrap")
colnames(results)<-c("bias","standard error")
results[1,1]<-biasbs
results[1,2]<-sebs

library(knitr)
knitr::kable(results,align=rep('c', 5))

```

\
\
**7.8:**
\
\
Use Jackknife method to estimate bias and standard error:
\

```{r}
# Jackknife
n<-nrow(scor)           #sample size
thetajk<-rep(0,n)
for (i in 1:n) {
x<-scor[-i,]
lambda<-eigen(cov(x))$values
thetajk[i]<-lambda[1]/sum(lambda)
}

biasjk<-(n-1)*(mean(thetajk)-thetaesti)
sejk<-(n-1)*sqrt(var(thetajk)/n)


results2<-matrix(0,nrow=1,ncol=2)
rownames(results2)<-c("Jackknife")
colnames(results2)<-c("bias","standard error")
results2[1,1]<-biasjk
results2[1,2]<-sejk


library(knitr)
knitr::kable(results2,align=rep('c',5))

```

\
\
**7.9:**
\
\
Compute the 95% percentile and BCa confidence intervals: 
\
 
```{r}
set.seed(2021)
bs<-function(data,index){
x<-data[index,]
lambda<-eigen(cov(x))$values
theta<-lambda[1]/sum(lambda)
return(theta)
}

bsr<-boot(data=SCOR,statistic=bs,R=1000)

bsCI<-boot.ci(bsr,conf=0.95,type=c("perc", "bca"))
print(bsCI)
```
\
\
**7.B:**
\
\
For normal distribution,the skewness equals to 0.
\
For $X \sim \chi ^{2}(5)$,$E(X)=5,Var(X)=10$,as
$${E}(X^{m}) = \frac{2^{m} \Gamma\left( \frac{k}{2}+m \right)}{\Gamma\left( \frac{k}{2} \right)}.$$
Therefore,${E}(X^{3}) = \frac{2^{3} \Gamma\left( \frac{5}{2}+3 \right)}{\Gamma\left( \frac{5}{2} \right)}=315.$ 
\
So $\mu_3=E[(X-EX)^3]=E(X^{3})-3E(X^{2})E(X)+2({E(X)})^3=40,S_k=\frac{\mu_3}{\sigma ^3}=\frac{40}{10^{\frac{3}{2}}}=\frac{4}{\sqrt {10}}$.
\
```{r}
library(boot)
set.seed(2021)
sk<-function(data,index){
x<-data[index]
xbar<-mean(x)
e3<-mean((x-xbar)^3)
e2<-mean((x-xbar)^2)
return(e3/(e2^1.5))
}
n<-50

s1<-0
s2<-4/sqrt(10)     #THe skewness of χ2(5) distributions
B<-500

normCI1<-basicCI1<-percentCI1<-matrix(0,B,2)
normCI2<-basicCI2<-percentCI2<-matrix(0,B,2)
for (i in 1:B){
  X<-rnorm(n)
  Y<-rchisq(n,5)
  boot.obj1<-boot(data=X,statistic=sk,R=500)
  boot.obj2<-boot(data=Y,statistic=sk,R=500)
  CI1<-boot.ci(boot.obj1,conf=0.95,type =c("norm","basic","perc"))
  CI2<-boot.ci(boot.obj2,conf=0.95,type=c("norm","basic","perc"))
normCI1[i,]<-CI1$norm[2:3]
basicCI1[i,]<-CI1$basic[4:5]
percentCI1[i,]<-CI1$percent[4:5]

normCI2[i,]<-CI2$norm[2:3]
basicCI2[i,]<-CI2$basic[4:5]
percentCI2[i,]<-CI2$percent[4:5]
}

```
\
```{r}
results3<-matrix(0,nrow=2,ncol=3)
rownames(results3)<-c("normal","chi-suqared(5)")
colnames(results3)<-c("standard normal ","basic","percentile")
results3[1,1]<-mean(normCI1[,1]<=s1 & normCI1[,2]>=s1)
results3[1,2]<-mean(basicCI1[,1]<=s1 & basicCI1[,2]>=s1)
results3[1,3]<-mean(percentCI1[,1]<=s1 & percentCI1[,2]>=s1)
results3[2,1]<-mean(normCI2[,1]<=s2 & normCI2[,2]>=s2)
results3[2,2]<-mean(basicCI2[,1]<=s2 & basicCI2[,2]>=s2)
results3[2,3]<-mean(percentCI2[,1]<=s2 & percentCI2[,2]>=s2)
```
\
The coverage rates for normal populations and $\chi ^{2}(5)$ distribution:
\
```{r}
library(knitr)
knitr::kable(results3,align=rep('c',5))
```
\
```{r}

results4<-matrix(0,nrow=2,ncol=3)
rownames(results4)<-c("normal","chi-suqared(5)")
colnames(results4)<-c("standard normal","basic","percentile")
results4[1,1]<-mean(normCI1[,1]>s1)
results4[1,2]<-mean(basicCI1[,1]>s1)
results4[1,3]<-mean(percentCI1[,1]>s1)
results4[2,1]<-mean(normCI2[,1]>s2)
results4[2,2]<-mean(basicCI2[,1]>s2)
results4[2,3]<-mean(percentCI2[,1]>s2)
```
\
the proportion of times that the confidence intervals miss on the left：
\
```{r}
knitr::kable(results4,align=rep('c',5))
```
\
```{r}

results5<-matrix(0,nrow=2,ncol=3)
rownames(results5)<-c("normal","chi-suqared(5)")
colnames(results5)<-c("standard normal bootstrap CI ","basic bootstrap CI","percentile bootstrap CI")
results5[1,1]<-mean(normCI1[,2]<s1)
results5[1,2]<-mean(basicCI1[,2]<s1)
results5[1,3]<-mean(percentCI1[,2]<s1)
results5[2,1]<-mean(normCI2[,2]<s2)
results5[2,2]<-mean(basicCI2[,2]<s2)
results5[2,3]<-mean(percentCI2[,2]<s2)
```
\
the proportion of times that the confidence intervals miss on the right：
```{r}
knitr::kable(results5,align=rep('c',5))
```

\
\

## title: "2021—11—04"

## Question

\
**8.2:**
\
Implement the bivariate Spearman rank correlation test for independence as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reportedby cor.test on the same samples.

\
**Discussion:**
\
Design experiments for evaluating the performance of the NN,energy, and ball methods in various situations.\
1.Unequal variances and equal expectations\
2.Unequal variances and unequal expectations\
3.Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)\
4.Unbalanced samples (say, 1 case versus 10 controls)\
Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).

\

## Answer

\

**8.2:**
\
\
Consider $X \sim U[0,2]$,$Y \sim Exp(1)$ which should be independent to implement the bivariate Spearman rank correlation test for independence.
\

```{r}
set.seed(2021)
n<-200
X<-runif(n,0,2)
Y<-rexp(n,1)


independence<-function(z,index) {
#dims contains dimensions of x and y
x<-z[,1]       
y<-z[index,2] 
return(cor(x,y,method="spearman"))
}


library(boot)
z<-matrix(cbind(X,Y),ncol=2)
boot.obj<-boot(data=z,statistic=independence,R=499,sim = "permutation")
tb<-c(boot.obj$t0, boot.obj$t)

print(paste("The proportion of insignificant permutation is",mean(abs(tb)>=abs(boot.obj$t0))))
print(paste("The p-value obtained by function cor.test() is",round(cor.test(X,Y,alternative="two.sided",method="spearman")$p.value,3)))

```
\
We cannot refuse the null hypothesis,therefore should regard the two samples are independent,which conforms with the ground truth.
\
\
Then,consider a special dependent situation where $X \sim U[0,2]$,$Y=-X$.

```{r}
set.seed(2021)
n<-200
X<-runif(n,0,2)
Z<-2-X


z<-matrix(cbind(X,Z),ncol=2)
boot.obj<-boot(data=z,statistic=independence,R=499,sim = "permutation")
tb<-c(boot.obj$t0, boot.obj$t)

print(paste("The proportion of insignificant permutation is",mean(abs(tb)>=abs(boot.obj$t0))))
print(paste("The p-value obtained by function cor.test() is",round(cor.test(X,Z,alternative="two.sided",method="spearman")$p.value,3)))

```
\
We have good reason to refuse the null hypothesis,therefore should regard  the two samples are not independent,which conforms with the ground truth.
\
\
**Discussion:**
\
\
(1) Unequal variances and equal expectations
\
```{r}
m<-500               #Permutation times
d<-2                 #The dimension of data
n1<-n2<-30           #The sample size of X and Y
R<-499               #The number of bootstrap replicates.
k<-2                 #The Nearest Neighbor parameter
n<-n1+n2             #Total sample size
N=c(n1,n2)
p.values<-p.values2<-p.values3<-p.values4<-matrix(NA,m,3)   #contains p-value for 3 methods.
alpha<-0.05          #Significance level

#NN method
Tn<-function(z,ix,sizes,k){
  n1<-sizes[1]; n2<-sizes[2]; n<-n1+n2
  if(is.vector(z))z<-data.frame(z,0);
  z<-z[ix,];
  NN<-nn2(data=z,k=k+1)
  block1<-NN$nn.idx[1:n1,-1]
  block2<-NN$nn.idx[(n1+1):n,-1]
  i1<-sum(block1<n1+.5)
  i2<-sum(block2>n1+.5)
  return((i1+i2)/(k*n))
}

eqdist.nn<-function(z,sizes,k){
  boot.obj<-boot(data=z,statistic=Tn,R=R,sim="permutation",sizes=sizes,k=k)
  ts<-c(boot.obj$t0,boot.obj$t)
  p.value<-mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}

set.seed(2021)
library(RANN)
library(energy)
library(Ball)

#Unequal variances and equal expectations
for(i in 1:m){
  X<-matrix(rnorm(n1*d,mean=0,sd=1),ncol=d)
  Y<-matrix(rnorm(n2*d,mean=0,sd=2),ncol=d)
  z<-rbind(X,Y)
  #NN method
  p.values[i,1]<-eqdist.nn(z,N,k)$p.value
  #energy methods
  p.values[i,2]<-eqdist.etest(z,sizes=N,R=R)$p.value
  #Ball method
  p.values[i,3]<-bd.test(x=X,y=Y,num.permutations=R,seed=i*2021)$p.value
}

power1<-colMeans(p.values<alpha)
results<-matrix(0,nrow=1,ncol=3)
colnames(results)<-c("NN method","energy method","Ball method")
rownames(results)<-"power"
results[1,1]<-power1[1]
results[1,2]<-power1[2]
results[1,3]<-power1[3]


library(knitr)
knitr::kable(results,align=rep('c', 5))
```
\
For samples with unequal variances and equal expectations,the Ball method is the most powerful method among 3.
\
\
(2) Unequal variances and unequal expectations
\
```{r}
for(i in 1:m){
  X2<-matrix(rnorm(n1*d,mean=0,sd=1),ncol=d)
  Y2<-matrix(rnorm(n2*d,mean=0.5,sd=1.5),ncol=d)
  z2<-rbind(X2,Y2)
  #NN method
  p.values2[i,1]<-eqdist.nn(z2,N,k)$p.value
  #energy methods
  p.values2[i,2]<-eqdist.etest(z2,sizes=N,R=R)$p.value
  #Ball method
  p.values2[i,3]<-bd.test(x=X2,y=Y2,num.permutations=R,seed=i*2021)$p.value
}


power2<-colMeans(p.values2<alpha)
results<-matrix(0,nrow=1,ncol=3)
colnames(results)<-c("NN method","energy method","Ball method")
rownames(results)<-"power"
results[1,1]<-power2[1]
results[1,2]<-power2[2]
results[1,3]<-power2[3]


library(knitr)
knitr::kable(results,align=rep('c', 5))
```

\
For samples with unequal variances and unequal expectations,the Ball method is most powerful among 3 methods.
\
\
3)Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodal distribution (mixture of two normal distributions)
\
```{r}
for(i in 1:m){
  X3<-matrix(rt(n1*d,df=1),ncol=d)
  y1=rnorm(n2*d,mean=0,sd=1);
  y2=rnorm(n2*d,mean=1,sd=2)
  r<-sample(c(1,0),n,replace =TRUE,prob=c(0.5,0.5))
  Y3<-matrix(r*y1+(1-r)*y2,ncol=d)
  z3<-rbind(X3,Y3)
  #NN method
  p.values3[i,1]<-eqdist.nn(z3,N,k)$p.value
  #energy methods
  p.values3[i,2]<-eqdist.etest(z3,sizes=N,R=R)$p.value
  #Ball method
  p.values3[i,3]<-bd.test(x=X3,y=Y3,num.permutations=R,seed=i*2021)$p.value
}


power3<-colMeans(p.values3<alpha)
results<-matrix(0,nrow=1,ncol=3)
colnames(results)<-c("NN method","energy method","Ball method")
rownames(results)<-"power"
results[1,1]<-power3[1]
results[1,2]<-power3[2]
results[1,3]<-power3[3]


library(knitr)
knitr::kable(results,align=rep('c', 5))
```
\
For samples of non-normal distributions,the energy method is most powerful among 3 methods.
\
\
(4) Unbalanced samples (say, 1 case versus 10 controls)
\
```{r}
N2=c(n1,2*n2)
for(i in 1:m){
  X4<-matrix(rnorm(n1*d,mean=0,sd=1),ncol=d)
  Y4<-matrix(rnorm(2*n2*d,mean=0.5,sd=1),ncol=d)
  z4<-rbind(X4,Y4)

  #NN method
  p.values4[i,1]<-eqdist.nn(z4,N2,k)$p.value
  #energy methods
  p.values4[i,2]<-eqdist.etest(z4,sizes=N2,R=R)$p.value
  #Ball method
  p.values4[i,3]<-bd.test(x=X4,y=Y4,num.permutations=R,seed=i*2021)$p.value
}


power4<-colMeans(p.values4<alpha)
results<-matrix(0,nrow=1,ncol=3)
colnames(results)<-c("NN method","energy method","Ball method")
rownames(results)<-"power"
results[1,1]<-power4[1]
results[1,2]<-power4[2]
results[1,3]<-power4[3]


library(knitr)
knitr::kable(results,align=rep('c', 5))
```

\
For unbalanced samples,the energy method is most powerful among 3 methods.
\
\
In conclusion,Ball method is more suitable when the scale is different.\
Energy method is more suitable when the location is different.\
Both generally outperforms the NN method,but they cannot uniformly beat each other. 

\
\

## title: "2021—11—11"
\

## Question
\
**9.3:**
\
\
Use the Metropolis-Hastings sampler to generate random variables from a
standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy$(\theta,\mu)$ distribution has density function
$$f(x)=\frac{1}{\pi \theta (1+[(x-\mu)/\theta]^2)},  -\infty<x<\infty,\theta>0.$$
The standard Cauchy has the Cauchy($\theta = 1$,$\mu = 0$) density. (Note that the standard Cauchy density is equal to the Student t density with one degree of freedom.)
\
\

**9.8:**
\
\
 This example appears in [40]. Consider the bivariate density
$$f(x,y)\propto\binom{n}{x}y^{x+a+1}(1-y)^{n-x+b-1},x=0,1,\cdots,n,0\leq y\leq 1$$
It can be shown that for fixed a, b, n, the conditional distributions are Binomial(n, y) and Beta(x + a, n − x + b). Use the Gibbs sampler to generate a chain with target joint density f(x, y).
\
\
\

For each of the above exercise, use the Gelman-Rubin method
to monitor convergence of the chain, and run the chain until it
converges approximately to the target distribution according to
\hat{R}< 1.2.
\

## Answer
\

**9.3:**
\
\
Choose $U[-1,1]$(which is symmetric and therefore qualified as Metropolis-Hastings sampler) as the proposal distribution of this exercise.
```{r}
set.seed(2021)

#The standard Cauchy distribution's density function

f<-function(x) {
  return(1/(pi*(1+x^2)))
}


m <- 1000                     #the number of samples
x <- numeric(m)                #the vector that stores samples
x[1]<-runif(1,-1,1)            #the first sample
k<-0              #k records the number of rejected candidate points.
burntime<-100    #discard the first 1000 samples
u<-runif(m)       #useful for acceptance probability. 


#generate the chain
for (i in 2:m) {
  xt<-x[i-1]
  y<-xt+runif(1,min=-1,max=1)
  num<-f(y)*dnorm(xt,mean=y,sd=1)          #the numerator
  den<-f(xt)*dnorm(y,mean=xt,sd=1)         #the denominator
  if (u[i]<=num/den) x[i]<-y 
  else {
    x[i]<-xt
    k<-k+1               #y is rejected 
  } 
}
print(paste("The rate of candidate points rejected is ",k/m))


```
\
In this example, approximately 17% of the candidate points are rejected, so
the chain is relatively efficient.
\
\
To see the generated sample as a realization of a stochastic process, we can plot the sample vs the time index.
\
The following code will display a partial plot starting at time index 1001 after discarding the burn-in time.
\
```{r}
X<-x[burntime+1:m]             #discard the first 1000 samples.
index<-burntime+1:m
plot(index,X,type="l",main="trace plot",ylab="random number")
abline(h=-5,lty =2,col="red")     
#to highlight the range where most samples from t(1) fall in.
abline(h=5,lty =2,col="red")
```
\
Note that for samples generated from t(1),most fall in $[-5,5]$,which is highlighted using red dot lines.
\
The trace plot above indicates good convergence of the chain.
\
Now plot the histogram of the chain to see if it converges to standard Cauchy.
\
```{r}
hist(X,breaks="scott",main="histogram  of the generated samples",xlab="",freq=FALSE,xlim=c(-10,10),ylim=c(0,0.35))
lines(k,dcauchy(k),col="red",lwd=1.5)
```

\
And the histogram of the generated samples confirms that.(the red line stands for the density of the standard Cauchy distribution.)
\
\
The following code compares the deciles of the generated observations with the deciles of the standard Cauchy distribution:
\
```{r}
Q<-quantile(X,probs=seq(0.1,0.9,0.1),na.rm=TRUE)
Qqc<-qcauchy(seq(0.1,0.9,0.1),loc=0,scale=1)
k<-seq(-10,10,0.01)


results<-matrix(0,nrow=9,ncol=2)
rownames(results)<-c("10%","20%","30%","40%","50%","60%","70%","80%","90%")
colnames(results)<-c("Generated observations","standard Cauchy distribution")
results[,1]<-Q
results[,2]<-Qqc

library(knitr)
knitr::kable(results,align=rep('c', 5))
```
\
\
Now we usse the Gelman-Rubin method to monitor convergence of the chain.
\
Note that for Cauchy distribution,the expectation doesn't exist,so monitoring the mean of the chain may not be a wise and robust method,instead,we can use median as the scalar summary statistic that estimates some parameter of the target distribution.
\
\
Target distribution: $t(1)$
\
Proposal distribution: $N(X_t,1)$
\
The scalar summary statistic of the target distribution $\phi_{ij}$: median of the $j$th chain up to time $i$.

```{r}
    Gelman.Rubin <- function(psi) {
        # psi[i,j] is the statistic psi(X[i,1:j])
        # for chain in i-th row of X
        psi <- as.matrix(psi)
        n <- ncol(psi)
        k <- nrow(psi)

        psi.means <- rowMeans(psi)     #row means
        B <- n * var(psi.means)        #between variance est.
        psi.w <- apply(psi, 1, "var")  #within variances
        W <- mean(psi.w)               #within est.
        v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
        r.hat <- v.hat / W             #G-R statistic
        return(r.hat)
        }

    Chain<-function(sigma,N,StartPoint) { 
#generates a Metropolis chain for t(1) with N(X[t],sigma) proposal distribution and deterimined starting value. 
        x <- rep(0, N)
        x[1] <-StartPoint
        u <- runif(N)

        for (i in 2:N) {
        xt<-x[i-1]
        y<-xt+runif(1,min=-1,max=1)
        num<-dt(y,1)*dnorm(xt,mean=y,sd=sigma)         
        den<-dt(xt,1)*dnorm(y,mean=xt,sd=sigma)
        s<-num/den
          if (u[i]<=s)    x[i]<-y 
          else            x[i]<-xt
        }
        return(x)
        }

    sigma<-1     #parameter of proposal distribution  N(X[t],sigma)
    k<-4         #number of chains to generate
    n<- 10000      #length of chains
    burntime <- 1000       #burn-in length

    #choose overdispersed initial values
    x0 <- c(-1,1,-4,4)

    #generate the chains
    set.seed(2021)
    X <- matrix(0, nrow=k, ncol=n)
    for (i in 1:k)
        X[i, ] <- Chain(sigma, n, x0[i])

    #compute diagnostic statistics : median
    ind<-1
    psi<-matrix(NA,nrow=k,ncol=n)
    #for a matrix 1 indicates rows, 2 indicates columns
    for (i in 1:k){
       for (ind in 1:n){
         Z<-as.matrix(X[i,1:ind])
     psi[i,ind]<-apply(Z,2,median)
     ind<-ind+1
       }
}
    #plot psi for the four chains
     for (i in 1:k)
      if(i==1){
        plot((burntime+1):n,psi[i, (burntime+1):n],ylim=c(-1,1), type="l",
            xlab='index', ylab=bquote(phi),main="trace plot")
      }else{
        lines((burntime+1):n,psi[i,(burntime+1):n],col=i)
    }

    
```
\
We can see that $\phi$ converges to 0,which is reasonable as t(1) is a symmetric distribution,so the median should be close to 0.
\
In light of this observation,we can say that the chain converges approximately to the target distribution.
\
\
Further consider the Gelman-Rubin statistic $\sqrt{\hat{R}}$,and follow the rule that the chain converges approximately to the target distribution when  $\sqrt{\hat{R}}<1.2$.
```{r}
    
    #plot the sequence of R-hat statistics
    rhat<-rep(0,n)
    for (j in (burntime+1):n)
        rhat[j]<-Gelman.Rubin(psi[,1:j])
    plot(rhat[(burntime+1):n], type="l", xlab="",ylab="R",main = "the Gelman-Rubin statistic ")
    abline(h=1.2,lty=2,col="red")
```
\
Therefore we can draw the conclusion that the chain finally converges to the target distribution.
\
\

**9.8:**
\
\
For the target joint density in the form :
$$f(x,y)\propto\binom{n}{x}y^{x+a+1}(1-y)^{n-x+b-1},x=0,1,\cdots,n,0\leq y\leq 1$$

the conditional distribution of $X$ and $Y$ is Binomial $(n,y)$ and Beta $(x + a, n − x + b)$,respectively.
\
The code using the Gibbs sampler to generate a chain with such target joint density is as follows.
\
```{r}
set.seed(2021)
N<-1000                    #length of chain
burntime<-100             #burn-in length
X<-matrix(0,N,2)            #bivariate sample
a<-1
b<-1
#Beta(1,1) is equivalent to U[0,1]
n<-25 
X[1,]<-c(0,0)              #initialize starting point

for (i in 2:N) {
x2 <- X[i-1, 2]
X[i, 1] <- rbinom(1,size=n,prob=x2)
x1<- X[i,1]
X[i, 2] <- rbeta(1,x1+a,n-x1+b)
}
```
Obtain the trace plot of $X$ and $Y$.
\
```{r}
x<-X[(burntime+1):N,]
plot(x[,1],main="trace plot for X and Y",type='l',lwd=0.7,xlab='index',ylab='Random numbers',col="red")
lines(x[,2],lwd=0.7,col="blue")
legend('topright',c("X","Y"),lwd=1,col=c("red","blue"),cex=1.2)

```
\
Further we can give the bivariate chain plot.
\
```{r}
plot(x, main="Bivariate chain generated by the Gibbs sampler",cex=0.5,xlab="X",ylab="Y",ylim=range(x[,2]))
```
\
The above figure exhibits the elliptical symmetry of the bivariate chain, with strong positive correlation.(printed after discarding the burn-in sample.)
\
\
Note that the mean of $X$ should be $ny$,and the mean of $Y$ should be $\frac{x+a}{n+a+b}$.
\
Then we can compare the mean of $X$ and $Y$ to see if they are close to theoretical value.
\
```{r}
print(paste("The mean of X and Y is ",round(colMeans(x)[1],4),"and ",round(colMeans(x)[2],4),",respectively."))
```
Note that n=25 in this case,and 25 $\times$ 0.5003 is close to 12.5113,which indicates that the chain converges well.
\
\
Finally,use the Gelman-Rubin statistic $\sqrt{\hat{R}}$,and follow the rule that the chain converges approximately to the target distribution when  $\sqrt{\hat{R}}<1.2$.
```{r}
set.seed(2021)
a<-1
b<-1
n<-25
k<-3                     #number of chains to generate
N<- 1000                #length of chains
burntime <- 100         #burn-in length
X1<-X2<-X3<-matrix(0,N,2)         #bivariate sample

 #initialize starting point
X1[1,]<-c(0,0)             
X2[1,]<-c(0.4,0.2)
X3[1,]<-c(0.3,0.6)

 Gelman.Rubin <- function(psi) {
        # psi[i,j] is the statistic psi(X[i,1:j])
        # for chain in i-th row of X
        psi <- as.matrix(psi)
        n <- ncol(psi)
        k <- nrow(psi)

        psi.means <- rowMeans(psi)     #row means
        B <- n * var(psi.means)        #between variance est.
        psi.w <- apply(psi, 1, "var")  #within variances
        W <- mean(psi.w)               #within est.
        v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
        r.hat <- v.hat / W             #G-R statistic
        return(r.hat)
        }

for (i in 2:N) {
x2 <- X1[i-1, 2]
X1[i, 1] <- rbinom(1,size=n,prob=x2)
x1<- X1[i,1]
X1[i, 2] <- rbeta(1,x1+a,n-x1+b)
}

for (i in 2:N) {
x2 <- X2[i-1, 2]
X2[i, 1] <- rbinom(1,size=n,prob=x2)
x1<- X2[i,1]
X2[i, 2] <- rbeta(1,x1+a,n-x1+b)
}

for (i in 2:N) {
x2 <- X3[i-1, 2]
X3[i, 1] <- rbinom(1,size=n,prob=x2)
x1<- X3[i,1]
X3[i, 2] <- rbeta(1,x1+a,n-x1+b)
}

Z1<-matrix(0,nrow=k,ncol=N)
Z2<-matrix(0,nrow=k,ncol=N)
```
Give the Gelman-Rubin statistic plots of X and Y,respectively.
\
```{r}
Z1[1,]<-X1[,1]
Z1[2,]<-X2[,1]
Z1[3,]<-X3[,1]

psi<- t(apply(Z1, 1, cumsum))
    for (i in 1:nrow(psi))
        psi[i,] <- psi[i,] / (1:ncol(psi))
 
rhat <- rep(0,N)
    for (j in (burntime+1):N)
        rhat[j] <- Gelman.Rubin(psi[,1:j])
    plot(rhat[(burntime+1):N], type="l", xlab="", ylab="R",main="the Gelman-Rubin statistic for X")
    abline(h=1.2, lty=2,col="red")
```

```{r}
Z2[1,]<-X1[,2]
Z2[2,]<-X2[,2]
Z2[3,]<-X3[,2]

psi2<- t(apply(Z2, 1, cumsum))
    for (i in 1:nrow(psi2))
        psi2[i,] <- psi2[i,] / (1:ncol(psi2))
 
rhat2 <- rep(0,N)
    for (j in (burntime+1):N)
        rhat2[j] <- Gelman.Rubin(psi2[,1:j])
    plot(rhat2[(burntime+1):N], type="l", xlab="", ylab="R",main="the Gelman-Rubin statistic for Y")
    abline(h=1.2, lty=2,col="red")

```
\
We can see that both tends to be below 1.2 as n increases.
\
Therefore we can believe that the bivariate chain converges to the target joint density well.

\
\
## title: "2021—11—18"
\

## Question

\
**11.3:**
\
\
 (a) Write a function to compute the $k^{th}$ term in,
 $$\sum_{k=0}^{\infty}\frac{(-1)^k}{k!2^k}\frac{\left \| a \right \|^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma (\frac{d+1}{2})\Gamma (k+\frac{3}{2})}{\Gamma (k+\frac{d}{2}+1)},$$
where $d ≥ 1$ is an integer,$a$ is a vector in $\mathbb{R}^d$, and $\left \| \cdot \right \|$ denotes the Euclidean norm.
\
Perform the arithmetic so that the coefficients can be computed for
(almost) arbitrarily large $k$ and $d$. (This sum converges for all $a \in \mathbb{R}^d$.
\
(b) Modify the function so that it computes and returns the sum.
\
(c) Evaluate the sum when $a = (1, 2)^T$ .
\
\

**11.5:**
\
\
Write a function to solve the equation
$$\frac{2\Gamma (\frac{k}{2})}{\sqrt{\pi(k-1)}(\frac{k-1}{2})}\int_{0}^{c_{k-1}}(1+\frac{u^2}{k-1})^{-\frac{k}{2}}du=\frac{2\Gamma (\frac{k+1}{2})}{\sqrt{\pi k}(\frac{k}{2})}\int_{0}^{c_{k}}(1+\frac{u^2}{k})^{-\frac{k+1}{2}}du$$
for a, where
$$c_k=\sqrt{  \frac{a^2k}{k+1-a^2}  }$$
Compare the solutions with the points $A(k)$ in Exercise 11.4.
\
\

**EM exercise:**
\
\
Suppose $T_1,\cdots ,T_n $ are i.i.d. samples drawn from the exponential distribution with expectation $λ$. Those values
greater than $\tau$ are not observed due to right censorship, so that
the observed values are $Y_i=T_iI(T_i≤τ)+τI(T_i>τ),i = 1,\cdots, n$.
Suppose $\tau = 1 $and the observed $Y_i$ values are as follows:
$$0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85$$
Use the E-M algorithm to estimate $λ$, compare your result with
the observed data MLE (note: $Y_i$ follows a mixture
distribution).
\
\

## Answer
\

**11.3:**
\
\
(a) As $k!=\Gamma(k+1)$,
 $$\frac{(-1)^k}{k!2^k}\frac{\left \| a \right \|^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma (\frac{d+1}{2})\Gamma (k+\frac{3}{2})}{\Gamma (k+\frac{d}{2}+1)},$$
 can be rewritten as $$\frac{(-1)^k}{2^k}\frac{\left \| a \right \|^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma (\frac{d+1}{2})\Gamma (k+\frac{3}{2})}{\Gamma (k+\frac{d}{2}+1)\Gamma (k+1)}$$
Use the idea that $\frac{b}{a}=exp(lg{\frac{b}{a}} )=exp(lgb-lga)$,we can compute the kth term in the following way:
\
```{r}

#compute the Euclidean norm of a vector
norm_vec<-function(x,k)    sqrt(sum(x*x)^(2*k+2))
#compute the kth term of the series
kth<-function(vector,d,k){
  stopifnot(d>=1) 
  term<-(-1)^k*exp(lgamma((d+1)/2)-lgamma(k+d/2+1)+lgamma(k+3/2)-lgamma(k+1))*norm_vec(a,k)/(2^k)/(2*k+1)/(2*k+2)

return (term)
}

```
\
(b) Modify the function so that it computes and returns the sum.
\
```{r}

#compute the sum of the series
series<-function(vector,d,eps){
  n<-0                           #the nth term
  total<-0                       #to store the sum
  repeat{
    total<-total+kth(vector,d,n)
    n<-n+1
    s<-abs(kth(vector,d,n))      #when   
  
  if (s<eps){
    break 
  }
  }
  return(total)
}

```
\
(c)Evaluate the sum when $a = (1, 2)^T$.
\
```{r}
eps<-.Machine$double.eps^0.25
a<-c(1,2)
d<-2

print(paste("The sum of the series is",round(series(a,d,eps),5),"when a=(1,2)T"))

```

\
**11.5:**
\
\
Set $k=4,5,\cdots,10.$Compare the solution with 11.4.
\
```{r}
K<-c(4:10)
N<-length(K)
root<-numeric(N)
eps<-.Machine$double.eps^0.25

for (n in 1:N) {
  k<-K[n]

  f<-function(a){
  z1<-sqrt(a^2*(k-1)/(k-a^2))
  z2<-sqrt(a^2*k/(k+1-a^2))
  f1<-function(x){
exp(lgamma(k/2)-lgamma((k-1)/2))*(1+(z1*x)^2 /(k-1))^(-k/2) *z1/sqrt((k-1)*pi)-exp(lgamma((k+1)/2)-lgamma(k/2))*(1+(z2*x)^2 /k)^(-(k+1)/2) *z2/sqrt(k*pi) }
  return(integrate(f1,eps,1)$value)
  }

 times<-0
 b0<-0
 b1<-sqrt(k)-eps
 r<-seq(b0, b1, length=3)
 y<-c(f(r[1]),f(r[2]),f(r[3]))
 if (y[1]*y[3] > 0)
stop("f does not have opposite sign at endpoints")
 while(times<1000 && abs(y[2])>eps) {
    times<-times + 1
if (y[1]*y[2]<0) {
  r[3]<-r[2]
  y[3]<-y[2]
        }
 else {
   r[1]<-r[2]
  y[1]<-y[2]
}
r[2]<-(r[1]+r[3])/2
y[2]<-f(r[2])

}

 root[n]<-r[2]
}
```
\
Present the solutions:
\
```{r}
# results<-matrix(0,nrow=7,ncol=1)
# rownames(results)<-4:10
# colnames(results)<-c("Solution")
# results[,1]<-root
# 
# library(knitr)
# knitr::kable(results,align=rep('c', 5))

```
\
For exercise 11.4:
\
```{r}
set.seed(2021)
K1<-c(4:25,100,500,1000)
N1<-length(K1)
eps<-.Machine$double.eps^0.25
root2<-numeric(N1)

#root-finding function,with regard to a
findinter<-function(a) {
  c1<-sqrt(a^2*(k-1)/(k-a^2))
  c2<-sqrt(a^2*k/(k+1-a^2))
  difference<-pt(c1,df=k-1)-pt(c2,df=k)
  #lower.tail=TRUE by default,computing P(X<=x)
  return (difference)
 }

for (n in 1:N1) {
  k<-K1[n]
  b0<-eps
  b1<-sqrt(k)-eps

 root2[n]<-uniroot(findinter,interval=c(b0, b1))$root
}

results2<-matrix(0,nrow=25,ncol=1)
rownames(results2)<-K1
colnames(results2)<-c("Intersection point")
results2[,1]<-root2

library(knitr)
knitr::kable(results2,align=rep('c', 5))
```
\
Note that when k reaches a certain level,the solution is always approximately $\sqrt{k}$,which is no longer reliable.But below a relatively low level,like $k=4,5,\cdots,10$,we can see the result is similar to the one of 11.5.
\
\

**EM exercise:**
\
\

Observed data: $Y_i,i=1,\cdots,10.$
\
Missed data: $T_i,i=1,\cdots,10.$
\
As the mean of $Exp(\lambda)$ is $\frac{1}{\lambda}$,$T_i,i=1,\cdots,n$ are drawn from $Exp(\frac{1}{\lambda})$.
\
\
According to $Y_i=T_iI(T_i≤τ)+τI(T_i>τ),i=1,\cdots,n$,for $Y_i<1$,we can actually know the corresponding missed data is itself.
\
\
Rearrange $Y_i$ so that the first 7 elements are less than 1,the last 3 elements are 1,for example:
$$(X_1,\cdots,X_{10})=(0.54,0.48,0.33,0.43,0.91,0.21,0.85,1,1,1).$$
\
\
Observed likelihood:
$$\begin{split}
L(\lambda|Y_1,\cdots,Y_{10})&=\prod_{i=1}^{7}[\frac{1}{\lambda}exp(-\frac{X_i}{\lambda})]\prod_{i=8}^{10}P(X_i>\tau)\\
&=\lambda^{-7}exp(\frac{-\sum_{i=1}^{7}X_i}{\lambda})exp(\frac{-3\tau}{\lambda})
\end{split}$$
$$ln L(\lambda|Y_1,\cdots,Y_{10})=-7ln\lambda-\frac{\sum_{i=1}^{7}X_i+3\tau }{\lambda}=-7ln\lambda-\frac{\sum_{i=1}^{10}Y_i}{\lambda}$$
$$\frac{\partial L(\lambda|Y_1,\cdots,Y_{10})}{\partial \lambda}=-\frac{7}{\lambda}+\frac{\sum_{i=1}^{10}Y_i}{\lambda ^2}.$$
\
Therefore,the MLE for observed data is $\hat{\lambda}=\frac{\sum_{i=1}^{10}Y_i}{7}$.
\
\
\
According to E-M algorithm,impute missed data $T$ with the conditional expectation $E(T|Y,\lambda^{(k)})$,which maximizes the log-likelihood of the complete data.
\
As said before,the first 7 $T_i$ is $Y_i$,so we just consider 
$E(T_j|\tau,\lambda^{(k)}).$
\
$$\begin{split}
E(T_j|\tau,\lambda^{(k)})&=\frac{E(T_jI(T_j>\tau) |\lambda^{(k)}  )}{P(T_j>\tau|\lambda^{(k)})} \\
&=\frac{\int_{\tau}^{+\infty}x\frac{1}{\lambda^{(k)}}exp(-\frac{x}{\lambda^{(k)}})dx    }{exp(-\frac{\tau}{\lambda^{(k)}})]}  \\
&=\frac{[\tau+ \lambda^{(k)}]exp(-\frac{\tau}{\lambda^{(k)}})}{exp(-\frac{\tau}{\lambda^{(k)}})]} \\
&=\tau+ \lambda^{(k)}
\end{split}$$.
\
Complete likelihood:
\
$$\begin{split}
L^*(\lambda)&=\lambda^{-10}exp(\frac{-\sum_{i=1}^{7}X_i}{\lambda})exp(\frac{-3[\tau+\lambda^{(k)}]}{\lambda})\\
&=\lambda^{-10}exp(\sum_{i=1}^{10}Y_i+3\lambda^{(k)})
\end{split}$$
$$lnL^*(\lambda)=-10ln\lambda-\frac{\sum_{i=1}^{10}Y_i+3\lambda^{(k)}}{\lambda}$$
Therefore,the MLE for complete data is $\hat{\lambda}^{(k+1)}=\frac{\sum_{i=1}^{10}Y_i+3\lambda^{(k)}}{10}$.
\
Assume the limit of $\lambda^{(k)}$ exists and equals to $A$,then  it  satisfies $A=\frac{\sum_{i=1}^{10}Y_i+3A}{10}$.
\
We can get $A=\frac{\sum_{i=1}^{10}Y_i}{7}$,which is just the MLE for observed data.
\
\
The following codes verify this conclusion.
\
```{r}
Y<-c(0.54,0.48,0.33,0.43,0.91,0.21,0.85,1,1,1)

#log-likelihood for observed data 
loglike<- function(lambda) {
        -7*log(lambda)-sum(Y)/lambda
}

#MLE for observed data 
lambdahat<-optimize(loglike,c(0,3),maximum = TRUE)$maximum

prior<-posterior<-lambdahat     #initialize lambda^(k)
eps<-1                

#log-likelihood for complete data 
conditional<-function(lambda) {
        -10*log(lambda)-(sum(Y)+3*prior)/lambda
}

#E-M iteration:
  while (eps>.Machine$double.eps^0.25) {
    prior<-posterior
    posterior<-optimize(conditional,c(0,2),maximum = TRUE)$maximum
    eps<-abs(posterior-prior)
  }


#Present the result
results3<-matrix(0,nrow=1,ncol=2)
rownames(results3)<-c("lambda")
colnames(results3)<-c("MLE for observed data","E-M result")
results3[,1]<-lambdahat
results3[,2]<-posterior

library(knitr)
knitr::kable(results3,align=rep('c', 5))

```

\
We can see that the result of E-M algorithm is very similar to the MLE of observed data.
\
\
## title: "2021—11—25"
\

## Question

\
**P204.1:**
\
\
Why are the following two invocations of lapply() equivalent?
\
trims<-c(0, 0.1, 0.2, 0.5)
\
x<-rcauchy(100)
\
lapply(trims, function(trim) mean(x, trim = trim))
\
lapply(trims,mean,x=x)
\
\

**P204.5:**
\
\
For each model in the previous two exercises, extract R2 using
the function below.
\
rsq <- function(mod) summary(mod)$r.squared
\
\

**P213.1:**
\
\
1. Use vapply() to:
\
a) Compute the standard deviation of every column in a numeric data frame.
\
b) Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply() twice.)

\
\

**P214.7:**
\
\
Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()?Why or why not?
\
\


## Answer
\

**P204.1:**
\
\
```{r}
trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(100)
#For conciseness,use unlist() to convert the output from a list to a vector.
#the trim option in mean() is the fraction (0 to 0.5) of observations to be trimmed from each end of x before the mean is computed. 
unlist(lapply(trims, function(trim) mean(x, trim = trim)))
unlist(lapply(trims, mean, x = x))

```
\
The result is the same.
\
\
The reason may be that:
\
The first code uses an anonymous function to vary the amount of trimming applied when computing the mean of a fixed x.
\
While the second code directly calls the mean function with its argument 1 "x" (which is assigned by "lapply" ) and argument 2 "trim" (which reflects the value of the current element).
\
Therefore they are equivalent.
\
\


**P204.5:**
\
\

```{r}
# Exercise 3
data("mtcars")
set.seed(2021)

#using the formulas stored in this list
formulas <- list( 
  mtcars$mpg ~ mtcars$disp,
  mtcars$mpg ~ I(1 / mtcars$disp),
  mtcars$mpg ~ mtcars$disp + mtcars$wt,
  mtcars$mpg ~ I(1 / mtcars$disp) + mtcars$wt
)
n<-length(formulas)

# loop
result<-list(n)
for (i in 1:n){
   result[[i]]<-summary(lm(formulas[[i]]))$r.squared
}
unlist(result)

# lapply
unlist(lapply(formulas,function(x){
  summary(lm(x))$r.squared
}))

```
\
Two methods derive the same result.
\
```{r}
# Exercise 4

bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})

# loop
result2<-list(10)
for(i in 1:10){
  result2[[i]]<-summary(lm(bootstraps[[i]]$mpg~bootstraps[[i]]$disp))$r.squared
}
unlist(result2)

# lapply
unlist(lapply(bootstraps,function(x){
      summary(lm(x$mpg~x$disp))$r.squared}))
```
\
Two methods derive the same result.
\
\

**P213.1:**
\
\
(a)
\
```{r}

set.seed(2021)
x<-runif(1000,0,1)
y<-rexp(1000,1)
df<-data.frame(x,y)
vapply(df,sd,numeric(1))
```
which is close to the standard deviation of U[0,1] (sd=$\sqrt{\frac{1}{12} } $) and Exp(1) (sd=1).
\
\
(b)
\
Take an easy mixed data frame for example.
\

```{r}
df2<-data.frame(x=1:5,y=c("a","b","c","d","e"))
vapply(df2[vapply(df2,is.numeric,logical(1))],sd,numeric(1))
```
\

**P214.7:**
\
\
The following codes implements a multi-core version of sapply().
\
Note that Windows needs to set up a local cluster first.
\
```{r}
# Error: processing vignette 'homework.Rmd' failed with diagnostics:
#  4 simultaneous processes spawned,so don't present the result
# library(parallel)
# options(warn=-1)           #neglect all the warnings
# cores<-detectCores()
# cluster<-makePSOCKcluster(cores)
# 
# mcsapply<-function(x,f){
#        out<-parSapply(cluster,x,f)
#        simplify2array(out)
#    }
# 
# #Take an easy problem for example.
# system.time(mcsapply(x=1:1e6,f=mean))
 
```
\
The following codes implements a multi-core version of vapply().
\
\
```{r}
# library(parallel)
# options(warn=-1)           #neglect all the warnings
# cores<-detectCores()
# cluster<-makePSOCKcluster(cores)
# 
# #FUN.VALUE=1.0 means the return value should be double(numeric) type.
# mcvapply<-function(x,f,FUN.VALUE) {
#        out_list<-parSapply(cluster,x,f)
#        out<-matrix(rep(FUN.VALUE,length(x)),nrow=length(x))
#        for (i in seq_along(x)){
#            out[i,]<-out_list[[i]]
#        }
#        out
# }
# 
# #Take an easy problem for example.
# system.time(mcvapply(x=1:1e6,f=mean,FUN.VALUE = 1.0))

```


\
\

## title: "2021—12—02"
\

## Question

\
**Extension of 9.8:**
\
\
 This example appears in [40]. Consider the bivariate density
$$f(x,y)\propto\binom{n}{x}y^{x+a+1}(1-y)^{n-x+b-1},x=0,1,\cdots,n,0\leq y\leq 1$$
It can be shown that for fixed a, b, n, the conditional distributions are Binomial(n,y) and Beta(x+a,n−x+b). Use the Gibbs sampler to generate a chain with target joint density f(x,y).
\
\
\

For each of the above exercise, use the Gelman-Rubin method
to monitor convergence of the chain, and run the chain until it
converges approximately to the target distribution according to
\hat{R}< 1.2.
\
\
Compare the corresponding generated random numbers with pure R language using the function “qqplot”.
\
\
Compare the computation time of the two functions with the
function “microbenchmark”.
\
\
Comments your results.
\
\

## Answer
\

**Rcpp function:**
\
\
```{r}
options(warn=-1)
library(Rcpp)
library(microbenchmark)

#Gibbs Sampler method using R
GibbsR<-function (a,b,N) {
X<-matrix(0,N,2)            
X[1,]<-c(0,0)                 #initialize       
for (i in 2:N) {
x2<-X[i-1,2]
X[i,1]<-rbinom(1,size=n,prob=x2)
x1<-X[i,1]
X[i,2]<-rbeta(1,x1+a,n-x1+b)
}
return (X)
}
```

```{r}
#Gibbs Sampler method using C

cppFunction(
  'NumericMatrix GibbsC (int a,int b,int N){
  NumericMatrix X(N,2); 
  X(0,0)=0;
  X(0,1)=0;
  double n=25;
  double x1=0;
  double x2=0;
  for (int i=1;i<N;i++){ 
    x2=X(i-1,1);
    X(i,0)=rbinom(1,n,x2)[0];
    x1=X(i,0);
    X(i,1)=rbeta(1,x1+a,n-x1+b)[0];
  }
  return X;
}')
```
\
**Comparison via qqplot():**
\
```{r}
a<-1       
b<-1
n<-25
N<-10000                 #length of chain

GR<-GibbsR(a,b,N)  
GC<-GibbsC(a,b,N)
#Binomial condition function
qqplot(GR[,1],GC[,1],xlab = "R function",ylab="C++ function",main= paste("a=",a,",b=",b,",n=",n,",Condition function:Binomial"))
#Beta condition function
qqplot(GR[,2],GC[,2],xlab = "R function",ylab="C++ function",main= paste("a=",a,",b=",b,",n=",n,",Condition function:Beta"))

```
\
**Comparison of running-time via microbenchmark():**
\
```{r}
ts<-microbenchmark(GibbsUsingR=GibbsR(a,b,N), GibbsUsingC=GibbsC(a,b,N))

library(knitr)
knitr::kable(summary(ts)[,c(1,3,4,5,6)],align=rep('c', 5))

```

\
Note that the unit of the result is millisecond.
\
\
**Comments:**
\
\
From the Q-Q plot, we can believe that the random numbers generated by two methods approximately follow the same distribution.What's more,both R method and C++ method (via rcpp function) yields satisfactory result. 
\
\
However,the running time of pure R function is much slower than the method using rcpp function,so consider using rcpp when faced with complex problems,especially when a loop is used.

\
\