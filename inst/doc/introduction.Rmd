---
title: "Introduction to StatComp21040"
author: 'Jiaqi Zhang'
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp21040}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview
\
Three functions are included and three numeric experiments are implemented in this package to demonstrate two methods for the detection of change point. _ChangePoint_ is implemented using the MCMC method, and the _Checkfunction1_ and _Checkfunction2_ are inspired by an intuitive idea, enable to deal with different types of change points. Specific procedures will be introduced later.
\
\
The numeric study consists of one simulation test and two real-data analysis. Notably, the ChangePoint is a universal method, Checkfunction1 is easy to implement and Checkfunction2 can detect possible change points in different perspectives. Notably, Checkfunction1 and Checkfunction2 can act as auxiliary tools for further improvement of ChangePoint. 
  
## _ChangePoint_

\
Suppose that the expected value of $Z_{i}$ is constant, being the same for the first $r$ observations but different for the last $N-r$ observations, i.e. the change point occurs at $r$.
\
Exact form of the model is
$$
Z_{i} \sim \begin{cases}N\left(\mu_{1}, \sigma^{2}\right), & 1 \leqslant i \leqslant r, \\ N\left(\mu_{2}, \sigma^{2}\right), & r<i \leqslant N,\end{cases}
$$
where $r, \mu_{1}, \mu_{2}$ and $\sigma^{2}$ are unknown random parameters to be estimated. 
\
\
Use Bayesian approach and MCMC method to estimate them. 
\
Notice that we will use the notation $Z_{i} \sim N\left(\mu_{i}, 1 / \gamma\right), i=1,2,\gamma=1 / \sigma^{2}$ for a simplification of result.

Assume that $r, \mu_{1}, \mu_{2}$ and $\gamma$ are independent random variables. $r$ follows a discrete uniform distribution on the set $\{1, \ldots, N\}, \quad \mathcal{L}\left(\mu_{1}\right) \sim N\left(\nu_{1}, \xi_{1}^{2}\right), \quad \mathcal{L}\left(\mu_{2}\right) \sim N\left(\nu_{2}, \xi_{2}^{2}\right),\quad \mathcal{L}(\gamma) \sim G a(1,1)$

\
Then the density of the prior distribution is 
$$f\left(\mu_{1}, \mu_{2}, \gamma, r\right) \propto \exp \left\{-\frac{1}{2}\left(\mu_{1}-\nu_{1}\right)^{2} / \xi_{1}^{2}-\frac{1}{2}\left(\mu_{2}-\nu_{2}\right)^{2} / \xi_{2}^{2}-\gamma\right\}$$
\
The likelihood function of the sequence $Z_{1}, \ldots, Z_{N}$ for given values of parameters is
$$
f\left(z_{1}, \ldots, z_{N} \mid \mu_{1}, \mu_{2}, \gamma, r\right) \propto \gamma^{N / 2} \exp \left(-\frac{\gamma}{2}\left[\sum_{i=1}^{r}\left(z_{i}-\mu_{1}\right)^{2}+\sum_{i=r+1}^{N}\left(z_{i}-\mu_{2}\right)^{2}\right]\right)
$$
and the posterior distribution:
$$
f_{\mu_{1}, \mu_{2}, \gamma, r \mid z} \propto \gamma^{N / 2} \exp \left(-\frac{\left(\mu_{1}-\nu_{1}\right)^{2}}{2 \xi_{1}^{2}}-\frac{\left(\mu_{2}-\nu_{2}\right)^{2}}{2 \xi_{2}^{2}}-\gamma-\frac{\gamma}{2}\left[\sum_{i=1}^{r}\left(z_{i}-\mu_{1}\right)^{2}+\sum_{i=r+1}^{N}\left(z_{i}-\mu_{2}\right)^{2}\right]\right),
$$
\
\
The MCMC algorithm generates each observation of the desired random sample in four steps as follows:
\
(1) Generate a candidate $r^{\prime}$ for new value of the parameter $r$ from $R\{1, \ldots, N\}$.
\
(2) Accept the candidate $r^{\prime}$ from step 1 with an appropriate probability, i.e. $r^{(n+1)}=r^{\prime}$ if accepted else $r^{(n+1)}=r^{(n)} .$
\
(3) Generate new values $\mu_{1}^{(n+1)}$ and $\mu_{2}^{(n+1)}$ from the conditional distribution $f_{\mu_{1}, \mu_{2}} \mid \gamma, r, z$, where the values $\gamma=\gamma^{(n)}$ and $r=r^{(n+1)}$ are given.
\
(4) Generate a new value $\gamma^{(n+1)}$ from the conditional distribution $f_{\gamma \mid \mu_{1}, \mu_{2}, r, \boldsymbol{z}}$, where the values $\mu_{1}=\mu_{1}^{(n+1)}, \mu_{2}=\mu_{2}^{(n+1)}$ and $r=r^{(n+1)}$ are given.
\
\
The density of the conditional distributions for steps 3 and 4 $f_{\mu_{1}, \mu_{2} \mid \gamma, r, z}$ has the form
$$
\begin{aligned}
f_{\mu_{1}, \mu_{2}} \mid \gamma, r, z \propto & \exp \left(-\left[\frac{\left(\mu_{1}-\nu_{1}\right)^{2}}{2 \xi_{1}^{2}}+\frac{\gamma}{2} \sum_{i=1}^{r}\left(z_{i}-\mu_{1}\right)^{2}\right]\right) \\
& \times \exp \left(-\left[\frac{\left(\mu_{2}-\nu_{2}\right)^{2}}{2 \xi_{2}^{2}}+\frac{\gamma}{2} \sum_{i=r+1}^{N}\left(z_{i}-\mu_{2}\right)^{2}\right]\right)
\end{aligned}
$$
\
Further, elementary calculations and substitutions $M_{1}(r)=\sum_{i=1}^{r} z_{i}$ and $M_{2}(r)=\sum_{i=r+1}^{N} z_{i}$ show that $f_{\mu_{1}, \mu_{2}} \mid \gamma, r, z$ is the density of a two-dimensional normal distribution with
the mean $\boldsymbol{\mu}$ and the variance matrix $\boldsymbol{\Sigma}$, where
$$
\boldsymbol{\mu}=\left(\begin{array}{c}
\frac{\nu_{1}+\gamma \xi_{1}^{2} M_{1}(r)}{1+\gamma \xi_{1}^{2} r} \\
\frac{\nu_{2}+\gamma \xi_{2}^{2} M_{2}(r)}{1+\gamma \xi_{2}^{2}(N-r)}
\end{array}\right) \quad \text { and } \quad \boldsymbol{\Sigma}=\left(\begin{array}{cc}
\frac{\xi_{1}^{2}}{1+\gamma \xi_{1}^{2} r} & 0 \\
0 & \frac{\xi_{2}^{2}}{1+\gamma \xi_{2}^{2}(N-r)}
\end{array}\right)
$$
\
The density $f_{\gamma \mid \mu_{1}, \mu_{2}, r, z}$ can be calculated analogously as above and we get
$$
f_{\gamma \mid \mu_{1}, \mu_{2}, r, z} \propto \gamma^{N / 2} \exp \left(-\gamma\left[1+\frac{1}{2} \sum_{i=1}^{r}\left(z_{i}-\mu_{1}\right)^{2}+\frac{1}{2} \sum_{i=r+1}^{N}\left(z_{i}-\mu_{2}\right)^{2}\right]\right),
$$
i.e. $f_{\gamma \mid \mu_{1}, \mu_{2}, r, z}$ is the density of a gamma distribution with the shape parameter $\frac{1}{2} N+1$ and the scale parameter $\left(1+\frac{1}{2}\left[\sum_{i=1}^{r}\left(z_{i}-\mu_{1}\right)^{2}+\sum_{i=r+1}^{N}\left(z_{i}-\mu_{2}\right)^{2}\right]\right)^{-1} .$
\
\
The only parameter we generate using the *Metropolis-Hastings algorithm* is the **change point $r$**. Notice that we do not use the Gibbs sampler due to the fact that the conditional distribution $f_{r \mid \mu_{1}, \mu_{2}, \gamma, z}$ is too complex to sample. Therefore, we always generate a candidate for $r$ and this is accepted with some probability in the next step. The formal way to fix this acceptance probability has been derived in,so we just state the right form of it. More precisely, we use $\alpha\left(\boldsymbol{x}^{(n)}, \boldsymbol{x}^{\prime}\right)=\min \left(1, \beta\left(\boldsymbol{x}^{(n)}, \boldsymbol{x}^{\prime}\right)\right)$, where
$$
\beta\left(\boldsymbol{x}^{(n)}, \boldsymbol{x}^{\prime}\right)=\frac{f_{\mu_{1}, \mu_{2}, \gamma, r \mid z}\left(\boldsymbol{x}^{\prime}\right)}{f_{\mu_{1}, \mu_{2}, \gamma, r \mid z}\left(\boldsymbol{x}^{(n)}\right)}
$$
Substituting the posterior density to it, we obtain the final form of the acceptance probability $\beta\left(\boldsymbol{x}^{(n)}, \boldsymbol{x}^{\prime}\right)$ equal to
$$
\begin{aligned}
\exp \left(\frac { \gamma ^ { ( n ) } } { 2 } \left[\sum_{i=1}^{r^{(n)}}\left(z_{i}-\mu_{1}^{(n)}\right)^{2}+\sum_{i=r^{(n)}+1}^{N}\left(z_{i}-\mu_{2}^{(n)}\right)^{2}\right.\right.&-\sum_{i=1}^{r^{\prime}}\left(z_{i}-\mu_{1}^{(n)}\right)^{2}\left.\left.-\sum_{i=r^{\prime}+1}^{N}\left(z_{i}-\mu_{2}^{(n)}\right)^{2}\right]\right)
\end{aligned}
$$
which can be equivalently written in the form
$$
\beta\left(\boldsymbol{x}^{(n)}, \boldsymbol{x}^{\prime}\right)=\left\{\begin{array}{l}
\exp \left(\frac { \gamma ^ { ( n ) } } { 2 } \left[\sum_{i=r^{(n)}}+1\right.\right. \qquad r^{\prime}>r^{(n)}\\
\exp \left(\frac{\gamma^{(n)}}{2}\left[\sum_{i=r^{\prime}+1}^{r^{(n)}}\left(z_{i}-\mu_{2}^{(n)}\right)^{2}-\sum_{i=r^{(n)}+1}^{r^{\prime}}\left(z_{i}-\mu_{1}^{(n)}\right)^{2}\right]\right) \qquad r^{\prime} \leqslant r^{(n)}\\
\end{array}\right.
$$
\
The R codes for ChangePoint is as follows: 
\
```{r}
ChangePoint<-function (y,v1,v2,e1,e2,N,burnin){
  mu1<-mu2<-sd1<-sd2<-GAMMA<-R<-prob<-cp<-numeric(N)
  n<-length(y) 
  prob[1]<-1
  R[1]<-sample(1:n,1)
  GAMMA[1]<-rgamma(1,1,1)
  m<-0
  u<-runif(N)
  sd1<-e1^2/(1+GAMMA[1]*e1^2*R[1])
  sd2<-e2^2/(1+GAMMA[1]*e2^2*(n-R[1]))
  mean1<-(v1+GAMMA[1]*e1^2*sum(y[1:R[1]]))/(1+GAMMA[1]*e1^2*R[1])
  mean2<-(v2+GAMMA[1]*e2^2*sum(y[(R[1]+1):n]))/(1+GAMMA[1]*e2^2*(n-R[1]))
  mu1[1]<-rnorm(1,mean=mean1,sd=sd1)
  mu2[1]<-rnorm(1,mean=mean2,sd=sd2)
for (i in 2:N) {
#generate the change point r using the Metropolis-Hastings algorithm for the complex conditional 
#distribution of r given other parameters is hard to sample from.
R[i]<-sample(1:n,1)
kt<-R[i-1]
if (kt<R[i]) {prob[i-1]<-exp(GAMMA[i-1]/2*(sum((y[(kt+1):R[i]]-mu2[i-1])^2)-sum((y[(kt+1):R[i]]-mu1[i-1])^2)))}
else {prob[i-1]<-exp(GAMMA[i-1]/2*(sum((y[min(R[i-1]+1,n):kt]-mu1[i-1])^2)-sum((y[min(R[i-1]+1,n):kt]-mu2[i-1])^2)))}

if (u[i]<=min(prob[i-1],1))  {r<-R[i];  m<-m+1;}
else {r<-kt}
cp[i]<-r;


if (r==n){
mean1<-(v1+GAMMA[i-1]*e1^2*sum(y[1:r]))/(1+GAMMA[i-1]*e1^2*r)
mean2<-v2+GAMMA[i-1]*e2^2*y[n]
sd1<-e1^2/(1+GAMMA[i-1]*e1^2*r)
sd2<-e2^2

mu1[i]<-rnorm(1,mean=mean1,sd=sd1)
mu2[i]<-rnorm(1,mean=mean2,sd=sd2)
z<-1+1/2*sum((y[1:r]-mu1[i])^2)+1/2*(y[n]-mu2[i])^2
GAMMA[i]<-rgamma(1,shape=n/2+1,rate=z)
}

else {
mean1<-(v1+GAMMA[i-1]*e1^2*sum(y[1:r]))/(1+GAMMA[i-1]*e1^2*r)
mean2<-(v2+GAMMA[i-1]*e2^2*sum(y[(r+1):n]))/(1+GAMMA[i-1]*e2^2*(n-r))
sd1<-e1^2/(1+GAMMA[i-1]*e1^2*r)
sd2<-e2^2/(1+GAMMA[i-1]*e2^2*(n-r))

mu1[i]<-rnorm(1,mean=mean1,sd=sd1)
mu2[i]<-rnorm(1,mean=mean2,sd=sd2)
z<-1+1/2*sum((y[1:r]-mu1[i])^2)+1/2*sum((y[(r+1):n]-mu2[i])^2)
GAMMA[i]<-rgamma(1,shape=n/2+1,rate=z)
}
}
  return(cp)[(burnin+1):N]
}
```

## _Checkfunction1_
\
The function Checkfunction1 can be used as a preliminary detection for possible change point. This function is inspired by the intuitive idea that **the slope of change point and its neighbor points should be large**. To simulate the different degrees of impact, I adopt the idea of kernel density estimation by computing a weighted mean of slopes. This is an easy and feasible method for many cases, like a prominent **"V" type** change point,**the piece-wise constant case with a sudden change** and so on. \
\
The R codes for Checkfunction1 is as follows: 
\
```{r}
Checkfunction1<-function (y,k,p){
  index1<-seq(k,1,-1)
  index2<-seq(1,k,1)
  weight1<-1/(2^index1)
  weight2<-1/(2^index2)
  weight<-c(weight1,0,weight2)
  n<-length(y)
  averageslope<-numeric(n)
  for (i in 1:n){
    if (i>k & i<n-k ){averageslope[i]<-weighted.mean(x=abs(y[(i-k):(i+k)]-y[i]),wt=weight/sum(weight))}
    else {averageslope[i]<--1 }
  }
  return(match(sort(averageslope,decreasing=TRUE),averageslope)[1:p])
  }
```

## _Checkfunction2_
\
However,for some certain types,for example: **a sudden continuing increase/decrease like a cliff**, function Checkfunction1 fails to detect the most obvious change point as the change point' slope might be smaller than the non-change ones' slopes. Therefore, I further improve the original algorithm **using the ratio of two near slopes to magnify the change** which are computed with k nearest points. 
\
\
This function manifests priority in detecting possible change point in both **macro and micro perspective** (i.e. the span of period considered, by adjusting the parameter k, which means the number of k nearest points).
\
\
The R codes for Checkfunction2 is as follows: 
\
```{r}
Checkfunction2<-function (y,k,p){
  n<-length(y)
  deviation<-slope1<-slope2<-ratio<-numeric(n)
  for (i in 1:n){
    if (i>k & i<n-k ) {slope1[i]<-(y[i]-y[i-k])/k
    slope2[i]<-(y[i+k]-y[i])/k
    ratio[i]<-(slope1[i]/slope2[i])
    }
  }
  for (i in 1:n){
    if (ratio[i]<0&abs(slope1[i])>median(abs(slope1))&abs(slope2[i])>median(abs(slope2)) )
    {deviation[i]<-max(ratio[i],1/ratio[i])+max(abs(slope1[i]/median(slope1[i])),abs(slope2[i]/median(slope2[i])))       }
    else if (ratio[i]>0) {deviation[i]<-max(ratio[i],1/ratio[i]) }
    else {deviation[i]<--0.001 }
  }
  return (match(sort(deviation,decreasing=TRUE),deviation)[1:p])
}
```
\

## _Numeric_ _Experiments_
\
\
**·Piece-wise constant case with a sudden change** 
\
```{r}
set.seed(1234)
x1<-rnorm(40,5,1)
x2<-rnorm(60,10,1)
y<-c(x1,x2)
plot(y,main="The overview of simulation data")
abline(v=length(x1),col="red",lwd=2,lty=2)
```
```{r}
s<-ChangePoint(y,v1=5,v2=10,e1=1,e2=1,N=10000,burnin=1000)
hist(s,freq=FALSE,main="histogram of the change point",xlab="")
dens<-density(s,bw = "nrd0",kernel='gaussian',window=kernel)
plot(dens,main="kernel estimation of the density of change point")
abline(v=dens$x[which.max(dens$y)],col="red")
abline(v=length(x1),col="blue")
results<-matrix(0,nrow=1,ncol=2)
rownames(results)<-"Change point"
colnames(results)<-c("Theoretical","Computed")
results[,1]<-40
results[,2]<-dens$x[which.max(dens$y)]

library(knitr)
knitr::kable(results,align=rep('c', 5))
```
\
We can see that the theoretical change point detected by ChangePoint almost has the highest density value.  
\
```{r}
Checkfunction1(y,3,2)
```
\
The Checkfunction1 successfully the two change points, showing its attribute to perform well in dealing with piece-wise constant situation.
\
\
**·"V" type change point case**
\
```{r}
Japan<-c(36.42584 ,36.59264 ,36.79944 ,37.03648 ,37.32840 ,37.67568 ,38.09784 ,38.60000 ,39.22192 ,39.97360 ,40.89944 ,42.04432 ,42.61632 ,45.65936 ,48.24248 ,30.53680 ,51.69000 ,56.83000 ,57.71000 ,61.15200 ,61.58300 ,62.42700 ,63.23400 ,64.00500 ,64.73900 ,65.43700 ,66.09900 ,66.72700 ,67.32200 ,67.88700 ,68.42400 ,68.93500 ,69.42600 ,69.89800 ,70.35100 ,70.78600 ,71.20000 ,71.59600 ,71.98000 ,72.35800 ,72.74000)
plot(x=1930:1970,y=Japan,main="The overview of life expectancy of Japan from 1930 to 1970",xlab="",ylab="Life expectancy (year)")
abline(v=which.min(Japan)+1930-1,col="red",lwd=2,lty=2)
```
```{r}
s<-ChangePoint(Japan,v1=40,v2=65,e1=2,e2=2,N=10000,burnin=1000)
hist(s,freq=FALSE,main="histogram of the change point",xlab="")
dens<-density(s,bw = "nrd0",kernel='gaussian',window=kernel)
plot(dens,xlab="",main="kernel estimation of the density of change point")
abline(v=dens$x[which.max(dens$y)],col="red")
abline(v=which.min(Japan),col="blue")
results<-matrix(0,nrow=1,ncol=2)
rownames(results)<-"Change point"
colnames(results)<-c("Theoretical","Computed")
results[,1]<-which.min(Japan)-1+1930
results[,2]<-round(dens$x[which.max(dens$y)]-1+1930)

library(knitr)
knitr::kable(results,align=rep('c', 5))
```

ChangePoint performs well in this case.
\
The change point of life expectancy of Japan occurs at **1945**,which corresponds to **Japan's surrender in the world war Ⅱ**.
\

```{r}
Checkfunction1(Japan,2,1)+1930-1
```
\
The change point is detected precisely by Checkfunction1.
\
\
**cliff type change point case**
\
```{r}
China<-c(0.1422 ,0.1792 ,0.2197 ,0.2255 ,0.2668 ,0.3112 ,0.3473 ,0.4056 ,0.8193 ,1.1076 ,1.2096 ,0.8505 ,0.6738 ,0.6579 ,0.6504 ,0.6908 ,0.7401 ,0.6035 ,0.6320 ,0.7540 ,0.9763 ,1.0712 ,1.1140 ,1.1336 ,1.1317 ,1.2774 ,1.3010 ,1.3998 ,1.5355 ,1.5471 ,1.4944 ,1.4561 ,1.5629 ,1.6253 ,1.7434 ,1.8577 ,1.9216 ,2.0229 ,2.1322 ,2.1269 ,2.1114 ,2.1847 ,2.2630 ,2.3971 ,2.5203 ,2.7060 ,2.7989 ,2.7814 ,2.6419 ,2.6144 ,2.6649 ,2.7056 ,2.9622 ,3.4555 ,3.9482 ,4.4159 ,4.8481 ,5.1847 ,5.5386 ,5.7939 ,6.2950 ,6.9223 ,7.0623 ,7.1506 ,7.1353 ,7.0003 ,6.8742 ,6.9812 ,7.2077 ,7.3163 ,7.4117)
plot(x=1950:2020,y=China,main="The overview of Annual CO2 emissions per capita of China from 1950 to 2020",xlab="",ylab="Annual CO2 emissions per capita (ton)")
abline(v=2001,col="red",lwd=2,lty=2)
```
```{r}
#Changepoint method
s<-ChangePoint(China,v1=1,v2=5,e1=1,e2=2,N=10000,burnin=1000)
hist(s,freq=FALSE,main="histogram of the change point",xlab="")
dens<-density(s,bw = "nrd0",kernel='gaussian',window=kernel)
plot(dens,main="kerne estimation of the density of changepoint")
abline(v=dens$x[which.max(dens$y)],col="red")
abline(v=50,col="blue")
results<-matrix(0,nrow=1,ncol=2)
rownames(results)<-"Change point"
colnames(results)<-c("Theoretical","Computed")
results[,1]<-2001
results[,2]<-round(dens$x[which.max(dens$y)]+1950)

library(knitr)
knitr::kable(results,align=rep('c', 5))
```


```{r}
#Checkfunction2 method
micro<-Checkfunction2(China,3,6)+1950-1
macro<-Checkfunction2(China,10,6)+1950-1
results<-matrix(0,nrow=2,ncol=6)
rownames(results)<-c("micro perspective","macro perspective")
results[1,]<-micro
results[2,]<-macro

library(knitr)
knitr::kable(results,align=rep('c', 5))
```
\
We can see Checkfunction2 returns not exactly the same possible change points with different settings of parameters.
\
\
In a microscopic perspective,i.e. consider a short period, using small k, the detection turns delicate,corresponds to many historical events affecting China's development of industry,and further affecting the CO2 emissions. 
\
But in a macroscopic perspective,i.e. consider a long span, using large k, the detection still turns useful. The years around **2001** are prominent, which corresponds to **China's participation in WTO**, and of course this is the most important affair affecting China's development in industry. 
\
```{r}
plot(x=1950:2020,y=China,main="The most possible 6 change points of CO2 emissions of China",xlab="",ylab="Annual CO2 emissions per capita (ton)")
abline(v=micro,col="green",lwd=1.5,lty=2)
```
```{r}
plot(x=1950:2020,y=China,main="The most possible 6 change points of CO2 emissions of China",xlab="",ylab="Annual CO2 emissions per capita (ton)")
abline(v=macro,col="blue",lwd=1.5,lty=2)
```
\
\
In summary, all three methods work well in the numeric experiments. But pros and cons exist for different methods.
\
\
ChangePoint is a general recommendation in detecting the most prominent change point. But as the model considered in the Changepoint is quite simple, it may not perform well in dealing with complex problems.
\
Checkfunction1 and  Checkfunction2 then can be used as assistance,which satisfies the detection of change points with different types. What's more, they can also be used to help decide the span to use a one-time Changepoint for some times, which well surely improve the preciseness.
\
\
